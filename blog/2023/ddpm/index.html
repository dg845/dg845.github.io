<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>The DDPM Model | Daniel Gu</title> <meta name="author" content="Daniel Gu"> <meta name="description" content="Notes on the DDPM diffusion generative model."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%B1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://dg845.github.io/blog/2023/ddpm/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "The DDPM Model",
      "description": "Notes on the DDPM diffusion generative model.",
      "published": "February 26, 2023",
      "authors": [
        {
          "author": "Daniel Gu",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Daniel Gu</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"></a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">about</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The DDPM Model</h1> <p>Notes on the DDPM diffusion generative model.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#hierarchical-variational-autoencoders">Hierarchical Variational Autoencoders</a></div> <div><a href="#markovian-hierarchical-variational-autoencoders">Markovian Hierarchical Variational Autoencoders</a></div> <ul> <li><a href="#deriving-an-elbo">Deriving an ELBO</a></li> <li><a href="#deriving-a-lower-variance-elbo">Deriving a Lower Variance ELBO</a></li> </ul> <div><a href="#denoising-diffusion-probabilistic-models">Denoising Diffusion Probabilistic Models</a></div> <ul> <li><a href="#simplifying-the-elbo">Simplifying the ELBO</a></li> <li><a href="#the-denoising-terms-l-1-t-1">The Denoising Terms $$L_{1:T - 1}$$</a></li> <li><a href="#learning-noise-instead-of-signal">Learning Noise instead of Signal</a></li> <li><a href="#the-reconstruction-term-l-0">The Reconstruction Term $$L_0$$</a></li> </ul> <div><a href="#summary-of-ddpm-math">Summary of DDPM Math</a></div> <div><a href="#implementing-a-ddpm-model">Implementing a DDPM Model</a></div> <ul> <li><a href="#model-architecture">Model Architecture</a></li> <li><a href="#variance-schedules">Variance Schedules</a></li> <li><a href="#forward-diffusion-process">Forward Diffusion Process</a></li> <li><a href="#training">Training</a></li> <li><a href="#sampling">Sampling</a></li> <li><a href="#metrics-and-evaluation">Metrics and Evaluation</a></li> </ul> <div><a href="#further-reading">Further Reading</a></div> <div><a href="#changelist">Changelist</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>This post covers the denoising diffusion probabilistic model as introduced in the paper “Denoising Diffusion Probabilistic Models” by Ho et al. <d-cite key="ho2020denoising"></d-cite>. Diffusion models were first introduced in “Deep Unsupervised Learning using Nonequilibrium Thermodynamics” by Sohl-Dickstein et al. <d-cite key="sohl2015deep"></d-cite>, but gained popularity after the DDPM paper demonstrated that diffusion models could generate high-quality images.</p> <p>Familiarity with generative models, particularly the variational autoencoder model, is assumed. A good resource on VAEs is <a href="https://bjlkeng.github.io/posts/variational-autoencoders/" rel="external nofollow noopener" target="_blank">Variational Autoencoders</a>, by Brian Keng <d-cite key="Keng2020vae"></d-cite>. I will not cover improvements to the basic DDPM model, guidance, score-based generative models, or applications here, although I hope to cover them in future posts.</p> <p>I am heavily indebted to <a href="https://calvinyluo.com/2022/08/26/diffusion-tutorial.html" rel="external nofollow noopener" target="_blank">Understanding Diffusion Models: A Unified Perspective</a> by Calvin Luo <d-cite key="luo2022understanding"></d-cite> for the presentation of the DDPM math and <a href="https://huggingface.co/blog/annotated-diffusion" rel="external nofollow noopener" target="_blank">The Annotated Diffusion Model</a> by Niels Rogge and Kashif Rasul <d-cite key="Rogge2022annotated"></d-cite> for learning about how to implement a DDPM. Be sure to check out those posts!</p> <p>The presentation of the math follows the overall structure of <a href="https://calvinyluo.com/2022/08/26/diffusion-tutorial.html" rel="external nofollow noopener" target="_blank">Understanding Diffusion Models: A Unified Perspective</a>; I have tried to present my own take on the derivations. The code snippets are based on the code from <a href="https://huggingface.co/blog/annotated-diffusion" rel="external nofollow noopener" target="_blank">The Annotated Diffusion Model</a> <d-cite key="Rogge2022annotated"></d-cite>, which is based on the <a href="https://github.com/lucidrains/denoising-diffusion-pytorch" rel="external nofollow noopener" target="_blank">PyTorch implementation by Phil Wang</a> <d-cite key="Wang2022denoising_impl"></d-cite>, which in turn is based on the <a href="https://github.com/hojonathanho/diffusion" rel="external nofollow noopener" target="_blank">original Tensorflow implementation</a> by the DDPM authors <d-cite key="ho2020denoising_impl"></d-cite>. An accompanying code repo can be found <a href="https://github.com/dg845/diffusion-models" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>If you aren’t interested in the nitty gritty of diffusion model math, you can skip down to the “Summary of DDPM Math” section, which contains a self-contained summary of the important equations for understanding and implementing a DDPM model.</p> <h2 id="hierarchical-variational-autoencoders">Hierarchical Variational Autoencoders</h2> <p>In a standard variational autoencoder (VAE) model, we have a set of data variables \(x\) and latent variables \(z\), and we are interested in modeling the marginal distribution of the data \(p(x)\). This is done through learning an encoder \(q_\phi(z \mid x)\) which is a variational approximation to the true posterior \(p(z \mid x)\) and a decoder \(p_\theta(x \mid z)\) which recovers the data distribution conditioned on the latent variables \(z\). This model is typically trained by maximizing a <em>variational lower bound (VLB)</em> or <em>evidence lower bound (ELBO)</em> <d-cite key="Keng2020vae"></d-cite>:</p> \[\begin{equation} \label{eq:vae_elbo} \log{p(x)} \geq \mathbb{E}_{z \sim q_\phi(z | x)}[\log{p_\theta(x | z)}] - D_{KL}(q_\phi(z | x) || p(z)) \end{equation}\] <p>A <em>hierarchical variational autoencoder (HVAE)</em> extends this model by introducing higher level latents \(z_1, z_2, \ldots, z_T\), where a higher level latent variable \(z_t\) is allowed to condition on all lower level latents \(z_1, \ldots, z_{t - 1}\). <d-cite key="luo2022understanding"></d-cite> As with a VAE, maximizing the log likelihood \(\log{\int{p(x, z_{1:T})dz_{1:T}}}\) of this model is generally intractable, but we can still derive an ELBO using a variational approximation \(q_\phi(z_{1:T} \mid x)\):</p> \[\begin{equation} \label{eq:hvae_elbo} \begin{split} \log{p(x)} &amp; = \log{\int{p(x, z_{1:T})dz_{1:T}}} \\ &amp; = \log{\int{\frac{p(x, z_{1:T})q_\phi(z_{1:T} | x)}{q_\phi(z_{1:T} | x)}dz_{1:T}}} \\ &amp; = \log{\mathbb{E}_{z_{1:T} \sim q_\phi(z_{1:T} | x)}[\frac{p(x, z_{1:T})}{q_\phi(z_{1:T} | x)}]} \\ &amp; \geq \mathbb{E}_{z_{1:T} \sim q_\phi(z_{1:T} | x)}[\log{\frac{p(x, z_{1:T})}{q_\phi(z_{1:T} | x)}}] \end{split} \end{equation}\] <p>The last line follows by <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" rel="external nofollow noopener" target="_blank">Jensen’s inequality</a> for a concave function \(f\): \(f(\mathbb{E}[x]) \geq \mathbb{E}[f(x)]\).</p> <p>To simplify \((\ref{eq:hvae_elbo})\) further, we need an expression for the joint distribution \(p(x, z_{1:T})\) and the posterior \(q_\phi(z_{1:T} \mid x)\).</p> <h2 id="markovian-hierarchical-variational-autoencoders">Markovian Hierarchical Variational Autoencoders</h2> <p>We will now make the following simplifying assumption: the latent variables \(z\) satisfy the Markov property as we move from higher-level latents to lower level latents. That is, we can express the decoding transition from \(z_{t+1}\) to $z_t$ as \(p(z_t \mid z_{t + 1})\). The generative process (starting from \(z_T\), whose prior distribution \(p(z_T)\) we specify by construction) is then a Markov chain, and we call a HVAE satisfying this property a <em>Markovian hierarchical variational autoencoder (MHVAE)</em>. This has a nice visual representation as follows:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ddpm_post/mhvae-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ddpm_post/mhvae-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ddpm_post/mhvae-1400.webp"></source> <img src="/assets/img/ddpm_post/mhvae.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> A visual representation of a MHVAE model. Image from <a href="https://calvinyluo.com/2022/08/26/diffusion-tutorial.html" rel="external nofollow noopener" target="_blank">Understanding Diffusion Models: A Unified Perspective</a>. </div> <p>Using the Markov property, we can write down the joint distribution of a MHVAE explicitly as</p> <p>\(\begin{equation} \label{eq:mhvae_joint_dist} p_\theta(x, z_{1:T}) = p_\theta(x | z_1)\prod_{t = 2}^{T}{p_\theta(z_{t - 1} | z_t)} \cdot p(z_T) \end{equation}\) Similarly, we can write the posterior distribution of the latent variables as</p> <p>\(\begin{equation} \label{eq:mhvae_post} q_\phi(z_{1:T} | x) = q_\phi(z_1 | x)\prod_{t = 2}^{T}{q_\phi(z_t | z_{t - 1})} \end{equation}\) To make the math simpler, we will make the following assumption:</p> <p><strong>Diffusion Model Latent Dimension Assumption.</strong> The dimension of the latent variables \(z_{1:T}\) equals the dimension of the data variable \(x\).</p> <p>We will thus rename the data variable to \(x_0\) and the latent variables to \(x_{1:T}\). With our new notation, we can rewrite the joint distribution as follows</p> <p>\(\begin{equation} \label{eq:reverse_diff_process} p_\theta(x_{0:T}) = p(x_T)\prod_{t = 1}^{T}{p_\theta(x_{t - 1} | x_t)} \end{equation}\) This Markov chain defines the <em>reverse diffusion process</em>, starting from the prior \(p(x_T)\). Similarly, for the posterior, we have</p> <p>\(\begin{equation} \label{eq:forward_diff_process} q_\phi(x_{1:T} | x_0) = \prod_{t = 1}^{T}{q_\phi(x_t | x_{t - 1})} \end{equation}\) This is also a Markov chain and defines the <em>forward diffusion process</em>. We will see how to interpret these terms later.</p> <h3 id="deriving-an-elbo">Deriving an ELBO</h3> <p>Now we can simplify our ELBO further:</p> \[\begin{equation} \label{eq:mhvae_elbo1_deriv} \begin{split} \log{p(x)} &amp; \geq \mathbb{E}_{x_{1:T} \sim q_\phi(x_{1:T} | x_0)}[\log\frac{p_\theta(x_{0:T})}{q_\phi(x_{1:T} | x_0)}] \\ &amp; = \mathbb{E}_{x_{1:T} \sim q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)\prod_{t = 1}^{T}{p_\theta(x_{t - 1} | x_t)}}{\prod_{t = 1}^{T}{q_\phi(x_t | x_{t - 1})}}] \\ &amp; = \mathbb{E}_{x_{1:T} \sim q_\phi(x_{1:T} | x_0)}[\log\frac{p_\theta(x_0 | x_1)\prod_{t = 2}^{T}{p_\theta(x_{t - 1} | x_t)} \cdot p(x_T)}{\prod_{t = 1}^{T - 1}{q_\phi(x_t |x_{t - 1})} \cdot q_\phi(x_T | x_{T - 1})}] \\ &amp; = \mathbb{E}_{x_{1:T} \sim q_\phi(x_{1:T} | x_0)}[\log\frac{p_\theta(x_0 | x_1)\prod_{t = 1}^{T - 1}{p_\theta(x_{t} | x_{t + 1})} \cdot p(x_T)}{\prod_{t = 1}^{T - 1}{q_\phi(x_t |x_{t - 1})} \cdot q_\phi(x_T | x_{T - 1})}] \\ &amp; = \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log{p_\theta(x_0 | x_1)}] + \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)}{q_\phi(x_T | x_{T - 1})}] \\ &amp; + \sum_{t = 1}^{T - 1}{\mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p_\theta(x_t | x_{t + 1})}{q_\phi(x_t | x_{t - 1})}]} \\ &amp; = \mathbb{E}_{q_\phi(x_1 | x_0)}[\log{p_\theta(x_0 | x_1)}] + \mathbb{E}_{q_\phi(x_{T - 1}, x_T | x_0)}[\log\frac{p(x_T)}{q_\phi(x_T | x_{T - 1})}] \\ &amp; + \sum_{t = 1}^{T - 1}{\mathbb{E}_{q_\phi(x_{t - 1}, x_t, x_{t + 1} | x_0)}[\log\frac{p_\theta(x_t | x_{t + 1})}{q_\phi(x_t | x_{t - 1})}]} \\ &amp; = \mathbb{E}_{q_\phi(x_1 | x_0)}[\log{p_\theta(x_0 | x_1)}] - \mathbb{E}_{q_\phi(x_{T - 1}, x_T | x_0)}[\log\frac{q_\phi(x_T | x_{T - 1})}{p(x_T)}] \\ &amp; - \sum_{t = 1}^{T - 1}{\mathbb{E}_{q_\phi(x_{t - 1}, x_t, x_{t + 1} | x_0)}[\log\frac{q_\phi(x_t | x_{t - 1})}{p_\theta(x_t | x_{t + 1})}]} \\ \end{split} \end{equation}\] <p>Since \(q_\phi(x_{t - 1}, x_t, x_{t + 1} \mid x_0) = q_\phi(x_t \mid x_{t -1}, x_{t + 1}, x_0)q_\phi(x_{t - 1}, x_{t + 1} \mid x_0)\) by the chain rule, which we can simplify to \(q_\phi(x_t \mid x_{t - 1})q_\phi(x_{t - 1}, x_{t + 1} \mid x_0)\) using the Markov property, we can rewrite the terms in the sum as a KL divergence:</p> \[\begin{equation} \label{eq:mhvae_elbo_1_kl_div_ex} \begin{split} L_t &amp; = \mathbb{E}_{q_\phi(x_{t - 1}, x_t, x_{t + 1})}[\log{\frac{q_\phi(x_t | x_{t - 1})}{p_\theta(x_t | x_{t + 1})}}] \\ &amp; = \mathbb{E}_{q_\phi(x_{t - 1}, x_{t + 1})}[D_{KL}(q_\phi(x_t | x_{t - 1}) || p_\theta(x_t | x_{t + 1}))] \\ \end{split} \end{equation}\] <p>Similarly \(q_\phi(x_{T - 1}, x_T \mid x_0) = q_\phi(x_{T - 1} \mid x_0)q_\phi(x_T \mid x_{T - 1})\), so we can rewrite the second and third terms as expectations over KL divergences as follows:</p> \[\begin{equation} \label{eq:vdm_elbo1} \begin{split} \log{p(x_0)} \geq &amp; \mathbb{E}_{q_\phi(x_1 | x_0)}[\log{p_\theta(x_0 | x_1)}] \\ &amp; - \sum_{t=1}^{T-1}{\mathbb{E}_{q_\phi(x_{t-1}, x_{t+1} | x_0)}[D_{KL}(q_\phi(x_t | x_{t-1}) || p_\theta(x_t | x_{t+1}))]} \\ &amp; - \mathbb{E}_{q_\phi(x_{T-1} | x_0)}[D_{KL}(q_\phi(x_T | x_{T-1}) || p(x_T))] \end{split} \end{equation}\] <p>Thus, the ELBO can be written as a sum of terms \(L_0 + L_1 + \ldots + L_t + \ldots + L_T\) which can be interpreted as follows:</p> <ol> <li> <strong>Reconstruction term</strong> \(L_0 = \mathbb{E}_{q_\phi(x_1 \mid x_0)}[\log{p_\theta(x_0 \mid x_1)}]\): this term expresses how well we are able to recover the original data \(x\) from the first level latent \(x_1\), and is the direct analogue of the reconstruction term in the VAE ELBO.</li> <li> <strong>Consistency terms</strong> \(L_t = \mathbb{E}_{q_\phi(x_{t-1}, x_{t+1} \mid x_0)}[D_{KL}(q_\phi(z_t \mid z_{t-1}) \vert\vert p_\theta(z_t \mid z_{t+1}))]\): these terms capture how close the noising transition from \(x_{t - 1}\) to \(x_t\) matches the denoising transition from \(x_{t + 1}\) to \(x_t\). Ideally, we want these to have the same distribution: we should end up with data at the same noise level whether we are going forwards or backwards.</li> <li> <strong>Prior Matching term</strong> \(L_T = \mathbb{E}_{q_\phi(x_{T - 1} \mid x_0)}[D_{KL}(q_\phi(x_T \mid x_{T-1}) \vert\vert p(x_T))]\): this captures how well our last forward transition \(q_\phi(x_T \mid x_{T - 1})\) matches the latent prior \(p(x_T)\), and is the direct analogue of the prior matching term in the VAE ELBO.</li> </ol> <p>The ELBO we derived also has a nice visual representation:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ddpm_post/mhvae_elbo1_crop-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ddpm_post/mhvae_elbo1_crop-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ddpm_post/mhvae_elbo1_crop-1400.webp"></source> <img src="/assets/img/ddpm_post/mhvae_elbo1_crop.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Each consistency term of the variational lower bound compares the KL divergence of the forward noising transition (pink arrow) and reverse denoising transition (green arrow); to maximize the VLB, we want to make these distributions as similar as possible. Image from <a href="https://calvinyluo.com/2022/08/26/diffusion-tutorial.html" rel="external nofollow noopener" target="_blank">Understanding Diffusion Models: A Unified Perspective</a>. </div> <p>We could try to directly optimize this ELBO, but because the consistency terms \(L_{1:T - 1}\) are expectations over two random variables \(x_{t-1}\) and \(x_{t+1}\), we would need to take a large number of samples to reduce their variance to an acceptable level.</p> <h3 id="deriving-a-lower-variance-elbo">Deriving a Lower Variance ELBO</h3> <p>Let us try to rewrite the ELBO such that all of our expectations are over only one random variable. We can begin the derivation as follows:</p> <p>\(\begin{equation} \label{eq:vdm_elbo2_deriv1} \begin{split} \log{p(x)} &amp; \geq \mathbb{E}_{x_{1:T} \sim q(x_{1:T} | x_0)}[\log\frac{p_\theta(x_{0:T})}{q_\phi(x_{1:T} | x_0)}] \\ &amp; = \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)\prod_{t = 1}^{T}{p_\theta(x_{t-1} | x_t)}}{\prod_{t = 1}^{T}{q_\phi(x_t | x_{t-1})}}] \\ &amp; = \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)p_\theta(x_0 | x_1)\prod_{t = 2}^{T}{p_\theta(x_{t-1} | x_t)}}{q_\phi(x_1 | x_0)\prod_{t = 2}^{T}{q_\phi(x_t | x_{t-1})}}] \end{split} \end{equation}\) Here we can make a key insight: due to the Markov property, we can write</p> <p>\(\begin{equation} \label{eq:vdm_elbo2_key} q_\phi(x_t | x_{t-1}) = q_\phi(x_t | x_{t-1}, x_0) = \frac{q_\phi(x_{t-1} | x_t, x_0)q_\phi(x_t | x_0)}{q_\phi(x_{t-1} | x_0)} \end{equation}\) Plugging this in to the product in the denominator of \((\ref{eq:vdm_elbo2_deriv1})\), we get:</p> <p>\(\begin{equation} \label{eq:vdm_elbo2_deriv2} \begin{split} \log{p(x)} &amp; \geq \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)p_\theta(x_0 | x_1)\prod_{t = 2}^{T}{p_\theta(x_{t-1} | x_t)}}{q_\phi(x_1 | x_0)\prod_{t = 2}^{T}{q_\phi(x_t | x_{t-1})}}] \\ &amp; = \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)p_\theta(x_0 | x_1)\prod_{t = 2}^{T}{p_\theta(x_{t-1} | x_t)}}{q_\phi(x_1 | x_0)\prod_{t = 2}^{T}{\frac{q_\phi(x_{t-1} | x_t, x_0)q_\phi(x_t | x_0)}{q_\phi(x_{t-1} | x_0)}}}] \\ \end{split} \end{equation}\) The \(\frac{q_\phi(x_t \mid x_0)}{q_\phi(x_{t-1} \mid x_0)}\) terms in the denominator product conveniently telescope to \(\frac{q_\phi(x_T \mid x_0)}{q_\phi(x_1 \mid x_0)}\), which leaves us with the following:</p> <p>\(\begin{equation} \label{eq:vdm_elbo2_deriv3} \begin{split} \log{p(x)} &amp; \geq \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)p_\theta(x_0 | x_1)\prod_{t = 2}^{T}{p_\theta(x_{t-1} | x_t)}}{q_\phi(x_1 | x_0)\prod_{t = 2}^{T}{\frac{q_\phi(x_{t-1} | x_t, x_0)q_\phi(x_t | x_0)}{q_\phi(x_{t-1} | x_0)}}}] \\ &amp; = \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)p_\theta(x_0 | x_1)\prod_{t = 2}^{T}{p_\theta(x_{t-1} | x_t)}}{q_\phi(x_T | x_0)\prod_{t = 2}^{T}{q_\phi(x_t | x_{t-1})}}] \\ &amp; = \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log{p_\theta(x_0 | x_1)}] + \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)}{q_\phi(x_T | x_0)}] \\ &amp; + \sum_{t=2}^{T}{\mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p_\theta(x_{t-1} | x_t)}{q_\phi(x_{t-1} | x_t, x_0)}]} \\ &amp; = \mathbb{E}_{q_\phi(x_1 | x_0)}[\log{p_\theta(x_0 | x_1)}] + \mathbb{E}_{q_\phi(x_T | x_0)}[\log\frac{p(x_T)}{q_\phi(x_T | x_0)}] \\ &amp; + \sum_{t=2}^{T}{\mathbb{E}_{q_\phi(x_{t-1}, x_t | x_0)}[\log\frac{p_\theta(x_{t-1} | x_t)}{q_\phi(x_{t-1} | x_t, x_0)}]} \\ &amp; = \mathbb{E}_{q_\phi(x_1 | x_0)}[\log{p_\theta(x_0 | x_1)}] - \mathbb{E}_{q_\phi(x_T | x_0)}[\log\frac{q_\phi(x_T | x_0)}{p(x_T)}] \\ &amp; - \sum_{t=2}^{T}{\mathbb{E}_{q_\phi(x_{t-1}, x_t | x_0)}[\log\frac{q_\phi(x_{t-1} | x_t, x_0)}{p_\theta(x_{t-1} | x_t)}]} \\ \end{split} \end{equation}\) We can immediately convert the second term \(\mathbb{E}_{q_\phi(x_T \mid x_0)}[\log\frac{q_\phi(x_T \mid x_0)}{p(x_T)}]\) into a KL divergence \(D_{KL}(q_\phi(x_T \mid x_0) \vert\vert p(x_T))\). For the terms in the third term sum, we can rewrite \(q_\phi(x_{t - 1}, x_t \mid x_0)\) as \(q_\phi(x_{t - 1} \mid x_t, x_0)q_\phi(x_t \mid x_0)\) using the chain rule of probability, so we can also turn these terms into a KL divergence inside an expectation over \(x_t \sim q(x_t \mid x_0)\): \(\mathbb{E}_{q_\phi(x_t \mid x_0)}[D_{KL}(q_\phi(x_{t-1} \mid x_t, x_0) \vert\vert p_\theta(x_{t-1} \mid x_t))]\). Thus we get the following ELBO:</p> <p>\(\begin{equation} \label{eq:vdm_elbo2} \begin{split} \log{p(x)} \geq &amp; \mathbb{E}_{q_\phi(x_1 | x_0)}[\log{p_\theta(x_0 | x_1)}] \\ &amp; - \sum_{t=2}^{T}{\mathbb{E}_{q_\phi(x_{t}| x_0)}[D_{KL}(q_\phi(x_{t-1} | x_t, x_0) || p_\theta(x_{t-1} | x_t))]} \\ &amp; - D_{KL}(q_\phi(x_T | x_0) || p(x_T)) \\ \end{split} \end{equation}\) As with the first ELBO \((\ref{eq:vdm_elbo1})\) we derived, we can write the ELBO as a sum of terms \(L_0 + L_1 + \ldots + L_{t - 1} + \ldots + L_T\):</p> <ol> <li> <strong>Reconstruction term</strong> \(L_0 = \mathbb{E}_{q_\phi(x_1 \mid x_0)}[\log{p_\theta(x_0 \mid x_1)}]\): this term remains unchanged from \((\ref{eq:vdm_elbo1})\), and captures how well we can reconstruct the original data \(x_0\) from the first latent variable \(x_1\).</li> <li> <strong>Denoising matching terms</strong> \(L_{t - 1} = \mathbb{E}_{q_\phi(x_t \mid x_0)}[D_{KL}(q_\phi(x_{t-1} \mid x_t, x_0) \vert\vert p_\theta(x_{t-1} \mid x_t))]\): these terms capture how close the denoising transition \(p_\theta(x_{t-1} \mid x_t)\) matches the “ground truth” denoising transition \(q_\phi(x_{t-1} \mid x_t, x_0)\). Since we have access to the original data \(x_0\), we can tractably recover \(x_{t - 1}\) in the ground truth denoising transition (also called the “forward process posterior”).</li> <li> <strong>Prior matching term</strong> \(L_T = D_{KL}(q_\phi(x_T \mid x_0) \vert\vert p(x_T))\): this expresses how well our forward sampling process \(q(x_T \mid x_0)\) matches the latent prior \(p(x_T)\), and is an analogue of the prior matching term in the VAE ELBO and our previous ELBO \((\ref{eq:vdm_elbo1})\).</li> </ol> <p>Similarly, we can visualize this ELBO:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ddpm_post/mhvae_elbo2_crop-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ddpm_post/mhvae_elbo2_crop-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ddpm_post/mhvae_elbo2_crop-1400.webp"></source> <img src="/assets/img/ddpm_post/mhvae_elbo2_crop.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Each denoising term of the variational lower bound compares the KL divergence of the forward process posterior (pink arrow; dependence on original data not shown) and reverse denoising transition (corresponding green arrow); to maximize the VLB, we want to make these distributions as similar as possible. Image from <a href="https://calvinyluo.com/2022/08/26/diffusion-tutorial.html" rel="external nofollow noopener" target="_blank">Understanding Diffusion Models: A Unified Perspective</a>. </div> <h2 id="denoising-diffusion-probabilistic-models">Denoising Diffusion Probabilistic Models</h2> <p>In this section we will focus on deriving a simple expression for the negative ELBO or variational lower bound (VLB), which in a slight abuse of notation we will call \(L_{VLB} = L_0 + L_1 + \ldots + L_{T - 1} + L_T\). <d-footnote>Each term is the negative of the corresponding term in the previous section, e.g. $$L_0 = -\mathbb{E}_{q_\phi(x_1 \mid x_0)}[\log{p_\theta(x_0 \mid x_1)}]$$</d-footnote>. The particular simplifying assumptions we make along the way give rise to the DDPM model.</p> <h3 id="simplifying-the-elbo">Simplifying the ELBO</h3> <p>We will make two further assumptions to simplify this ELBO:</p> <p><strong>Diffusion Model Encoder Assumption.</strong> The latent encoders \(q_\phi(x_t \mid x_{t-1})\) are fixed and linear Gaussian; that is, each encoding step is a Gaussian centered on the output of the previous encoding step: \(\begin{equation} \label{eq:diff_encoder_assumption} q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t}x_{t-1}, (1 - \alpha_t)I) \end{equation}\) where the noise parameters ${\alpha_t}$ are fixed.</p> <p>Alternatively, we could parameterize the approximate posterior in terms of a variance schedule \(\{\beta_t\}\) as \(q(x_t \mid x_{t - 1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t}x_0, \beta_tI)\), with \(\alpha_t = 1 -\beta_t\). We will stick to using only using \(\alpha_t\)s in our following derivation, but in papers it is common to see a mix of \(\alpha_t\)s and \(\beta_t\)s. <d-cite key="ho2020denoising,nichol2021improved"></d-cite></p> <p>This means we no longer have to learn the parameters \(\phi\) for the encoders, which makes our learning task (and math) easier, and we will drop the \(\phi\) subscripts from now on. Furthermore, since the variances are fixed to constants, the prior matching term \(L_T = D_{KL}(q(x_T \mid x_0) \vert\vert p(x_T))\) has no learnable parameters and can be ignored during training. <d-cite key="ho2020denoising"></d-cite></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ddpm_post/ddpm_prob_model-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ddpm_post/ddpm_prob_model-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ddpm_post/ddpm_prob_model-1400.webp"></source> <img src="/assets/img/ddpm_post/ddpm_prob_model.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The DDPM probabilistic graphical model. (Figure 2 from the <a href="https://arxiv.org/pdf/2006.11239.pdf" rel="external nofollow noopener" target="_blank">the DDPM paper</a>). </div> <p><strong>Diffusion Model Prior Assumption.</strong> The latent encoder variances \(\{\alpha_t\}\) vary such that the distribution \(p(x_T)\) of the final latent variable \(x_T\) is a standard Gaussian \(\mathcal{N}(0, I)\).</p> <p>This choice makes the prior matching term \(L_T\) approximately 0, as long as \(q(x_T \mid x_0) \approx \mathcal{N}(0, I)\) (that is, the last latent \(x_T\) is totally noisy). <d-cite key="nichol2021improved"></d-cite></p> <p>Now we can interpret what the forward and reverse diffusion processes do. In the forward diffusion process, we start from the original data \(x_0\) and gradually add noise until it becomes pure Gaussian noise at time \(t = T\). In the reverse diffusion process, we start from pure Gaussian noise \(x_T \sim \mathcal{N}(0, I)\) and perform a series of denoising steps \(p_\theta(x_{t - 1} \mid x_t)\) until we (approximately) recover the original data \(x_0\).The dimension, encoder, and prior assumptions are what differentiate a diffusion model from a general MHVAE.</p> <p>Using the prior assumption, the prior matching term vanishes, so we can write our simpler ELBO as</p> <p>\(\begin{equation} \label{eq:vdm_elbo2_simp} \begin{split} \log{p(x)} \geq &amp; \mathbb{E}_{q(x_1 | x_0)}[\log{p_\theta(x_0 | x_1)}] \\ &amp; - \sum_{t=2}^{T}{\mathbb{E}_{q(x_{t}| x_0)}[D_{KL}(q(x_{t-1} | x_t, x_0) || p_\theta(x_{t-1} | x_t))]} \end{split} \end{equation}\) By Bayes’ rule, we can invert our key insight \((\ref{eq:vdm_elbo2_key})\):</p> <p>\(\begin{equation} \label{eq:vdm_elbo2_key_inv} q(x_{t-1} | x_t, x_0) = \frac{q(x_t | x_{t-1}, x_0)q(x_{t-1} | x_0)}{q(x_t | x_0)} = \frac{q(x_t | x_{t-1})q(x_{t-1} | x_0)}{q(x_t | x_0)} \end{equation}\) where the second equality holds due to the Markov property. Since we already know \(q(x_t \mid x_{t-1})\) by the encoder assumption \((\ref{eq:diff_encoder_assumption})\), we only need to figure out \(q(x_t \mid x_0)\).</p> <p>Using the reparameterization trick for Gaussian distributions, we can write \(q(x_t \mid x_{t-1})\) as</p> <p>\(\begin{equation} \label{eq:reparam_trick} q(x_{t} | x_{t-1}) = \sqrt{\alpha_t}x_{t-1} + \sqrt{1 - \alpha_t}\epsilon_{t-1} \end{equation}\) where \(\epsilon_{t-1}\) is a standard Gaussian. Rewriting \(x_{t-1}\) with the reparameterization trick as well results in:</p> <p>\(\begin{equation} \label{eq:reparam_ind1} \begin{split} x_t &amp; = \sqrt{\alpha_t}x_{t-1} + \sqrt{1 - \alpha_t}\epsilon_{t-1} \\ &amp; = \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}x_{t-2} + \sqrt{1 - \alpha_{t-1}}\epsilon_{t-2}) + \sqrt{1 - \alpha_t}\epsilon_{t-1} \\ &amp; = \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{\alpha_t(1 - \alpha_{t-1})}\epsilon_{t-2} + \sqrt{1 - \alpha_t}\epsilon_{t-1} \\ \end{split} \end{equation}\) Since the sum of two independent Gaussian random variables is again a Gaussian random variable whose mean is the sum of the means and whose variance is the sum of the variances <d-footnote>See e.g. https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables.</d-footnote>, we can simplify this to</p> <p>\(\begin{equation} \label{eq:reparam_ind2} \begin{split} x_t &amp; = \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{\alpha_t(1 - \alpha_{t-1})}\epsilon_{t-2} + \sqrt{1 - \alpha_t}\epsilon_{t-1} \\ &amp; = \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{(\sqrt{\alpha_t(1 - \alpha_{t-1})})^2 + (\sqrt{1 - \alpha_t})^2}\epsilon_{t-2} \\ &amp; = \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{1 - \alpha_t\alpha_{t-1}}\epsilon_{t-2} \\ &amp; = \ldots \\ &amp; = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon_0 \end{split} \end{equation}\) where \(\bar{\alpha}_t = \prod_{i = 1}^{t}{\alpha_i}\).</p> <p>Since we have a closed form expression for the forward sampling process \(q(x_t \mid x_0)\) in terms of the original data \(x_0\), which we know, this means that we don’t have to run the diffusion process forward one step at a time; instead, we can directly sample \(x_t\) from \(q(x_t \mid x_0) \sim \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)I)\).</p> <h3 id="the-denoising-terms-l_1t---1">The Denoising Terms \(L_{1:T - 1}\)</h3> <p>Continuing our derivation for ground truth posterior \(q(x_{t-1} \mid x_t, x_0)\) we can then plug this into \((\ref{eq:vdm_elbo2_key_inv})\) to get</p> <p>\(\begin{equation} \begin{split} q(x_{t-1} | x_t, x_0) &amp; = \frac{q(x_t | x_{t-1})q(x_{t-1} | x_0)}{q(x_t | x_0)} \\ &amp; = \frac{\mathcal{N}(x_t; \sqrt{\alpha_t}, (1 - \alpha_t)I)\mathcal{N}(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}}, (1 - \bar{\alpha}_{t-1})I)}{\mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}, (1 - \bar{\alpha}_t)I)} \\ &amp; \propto \mathcal{N}(x_{t-1}; \frac{\sqrt{\alpha_{t}}(1 - \bar{\alpha}_{t-1})x_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)x_0}{1 - \bar{\alpha}_t}, \frac{(1 - \alpha_t)(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}) \end{split} \end{equation}\) Define</p> \[\begin{equation} \label{eq:denoising_mean} \mu_q(x_t, x_0) = \frac{\sqrt{\alpha_{t}}(1 - \bar{\alpha}_{t-1})x_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)x_0}{1 - \bar{\alpha}_t} \end{equation}\] <p>\(\begin{equation} \label{eq:denoising_var} \sigma_q^2(t) = \frac{(1 - \alpha_t)(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \end{equation}\) Since we know that our ground truth denoising transition \(q(x_{t-1} \mid x_t, x_0)\) is a Gaussian with fixed covariance $\sigma_q^2(t)I$ (which only depends on the fixed \(\{\alpha_t\}\)), we can model our denoising step \(p_\theta(x_{t-1} \mid x_t)\) as the following:</p> <p>\(\begin{equation} \label{eq:denoising_trans_dist} p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_q^2(t)I) \end{equation}\) where \(\mu_\theta(x_t, t)\) is a function approximator (e.g. a neural network) of the denoising mean, parameterized by \(\theta\).</p> <p>The KL divergence of two multivariate Gaussians \(\mathcal{N}_0 = \mathcal{N}(\mu_0, \Sigma_0)\) and \(\mathcal{N}_1 = \mathcal{N}(\mu_1, \Sigma_1)\) can be expressed in closed form as</p> <p>\(\begin{equation} \label{eq:mult_norm_kl_div} D_{KL}(\mathcal{N}_0 || \mathcal{N}_1) = \frac{1}{2}(\mbox{tr}(\Sigma_1^{-1}\Sigma_0) + (\mu_1 - \mu_0)^{T}\Sigma_1^{-1}(\mu_1 - \mu_0) + \ln\frac{\det{\Sigma_1}}{\det{\Sigma_0}}) \end{equation}\) Using the fact that the covariances are equal, we see that the expression collapses into something that looks like a reconstruction loss:</p> <p>\(\begin{equation} \label{eq:kl_div_denoise} D_{KL}(q(x_{t-1} | x_t, x_0) || p_\theta(x_{t-1} | x_t)) = \frac{1}{2\sigma_q^2}||\mu_\theta - \mu_q(x_t, x_0)||_2^2 \end{equation}\) We can further simplify our function approximator \(\mu_\theta(x_t, t)\) by parameterizing it to match the form of our ground truth mean given by \((\ref{eq:denoising_mean})\):</p> <p>\(\begin{equation} \label{denoising_mean_fn_approx} \mu_\theta(x_t, t) = \frac{\sqrt{\alpha_{t}}(1 - \bar{\alpha}_{t-1})x_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)\hat{x}_\theta(x_t, t)}{1 - \bar{\alpha}_t} \end{equation}\) where \(\hat{x}_\theta(x_t, t)\) tries to predict the original data \(x_0\) from a noisified version \(x_t\) and a time index \(t\). We can then express the denoising matching loss \(L_{1:T-1}\) as</p> <p>\(\begin{equation} \label{eq:loss_data_recon_weighted} \begin{split} L_{1:T - 1} &amp; = \sum_{t=2}^{T}{\mathbb{E}_{q(x_t | x_0)}[D_{KL}(q(x_{t-1} | x_t, x_0) || p_\theta(x_{t-1} | x_t))]} \\ &amp; = \sum_{t = 2}^{T}{\mathbb{E}_{q(x_t | x_0)}[\frac{1}{2\sigma_q^2}\frac{\bar{\alpha}_{t-1}(1 - \alpha_t)^2}{(1 - \bar{\alpha}_t)^2}||\hat{x}_\theta(x_t, t) - x_0||_2^2]} \end{split} \end{equation}\) Our optimization problem for the denoising terms then becomes</p> \[\begin{equation} \label{eq:loss_source_denoise_opt} \begin{split} &amp; \quad \underset{\theta}{\arg\min}\, \sum_{t = 2}^{T}{\mathbb{E}_{q(x_t | x_0)}[\frac{1}{2\sigma_q^2}\frac{\bar{\alpha}_{t-1}(1 - \alpha_t)^2}{(1 - \bar{\alpha}_t)^2}||\hat{x}_\theta(x_t, t) - x_0||_2^2]} \\ &amp; = \underset{\theta}{\arg\min}\, \mathbb{E}_{t \sim U\{2, T\}}[\mathbb{E}_{q(x_t | x_0)}[\frac{1}{2\sigma_q^2}\frac{\bar{\alpha}_{t-1}(1 - \alpha_t)^2}{(1 - \bar{\alpha}_t)^2}||\hat{x}_\theta(x_t, t) - x_0||_2^2]] \\ \end{split} \end{equation}\] <p>We can approximate the expectations with samples: first, we sample random noise levels \(t\), and then we randomly sample \(x_t \sim q(x_t \mid x_0)\), whose distribution is fixed and can be calculated in closed form by \((\ref{eq:reparam_ind2})\). Finally, we can use the sampled \(x_t\) and \(t\) to calculate a Monte Carlo estimate of the loss via \(\vert\vert\hat{x}_\theta(x_t, t) - x_0\vert\vert_2^2\).</p> <h3 id="learning-noise-instead-of-signal">Learning Noise Instead of Signal</h3> <p>Recall that by equation \((\ref{eq:reparam_ind2})\), we have</p> \[x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon_0\] <p>where \(x_0\) is the original data and \(\epsilon_0 \sim \mathcal{N}(\epsilon; 0, I)\) is the “source noise”. We can rearrange this in terms of \(x_0\) to get</p> \[\begin{equation} \label{eq:reparam_ind_noise} x_0 = \frac{x_t - \sqrt{1 - \bar{\alpha}}\epsilon_0}{\sqrt{\bar{\alpha}_t}} \end{equation}\] <p>We can use this expression to reparameterize our ground truth denoising mean \(\mu_q(x_t, x_0)\) as a function of \(x_t\) and \(\epsilon_0\):</p> \[\begin{equation} \label{eq:denoising_mean_noise} \begin{split} \mu_q(x_t, \epsilon_0) &amp; = \frac{\sqrt{\alpha_{t}}(1 - \bar{\alpha}_{t-1})x_t + \sqrt{\bar{\alpha}_{t-1}(1-\alpha_t)}x_0}{1 - \bar{\alpha}_t} \\ &amp; = \frac{1}{\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{(1 - \bar{\alpha}_t)(\alpha_t)}}\epsilon_0 \end{split} \end{equation}\] <p>Analogously, we can set our function approximation \(\mu_\theta(x_t, t)\) to have the form</p> \[\begin{equation} \label{eq:denoising_mean_noise_fn_approx} \mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{(1 - \bar{\alpha}_t)(\alpha_t)}}\hat{\epsilon}_\theta(x_t, t) \end{equation}\] <p>So we can rewrite out denoising matching loss \(L_{1:T - 1}\) as</p> <p>\(\begin{equation} \label{eq:loss_noise_recon} L_{1:T - 1} = \mathbb{E}_{t \sim U\{2, T\}}[\frac{1}{2\sigma_q^2}\frac{(1 - \alpha_t)^2}{(1 - \bar{\alpha}_t)\alpha_t}\mathbb{E}_{q(x_t | x_0)}[||\hat{\epsilon}_\theta(x_t, t) - \epsilon_0||_2^2]] \end{equation}\) where we learn to predict the source noise \(\epsilon_0\) using our function approximator \(\hat{\epsilon}_\theta(x_t, t)\).</p> <p>Although learning a model \(\hat{x}_\theta(x_t, t)\) to reconstruct the original data \(x_0\) and learning a model \(\hat{\epsilon}_\theta(x_t, t)\) to reconstruct the source noise \(\epsilon_0\) are mathematically equivalent, the DDPM paper found that modeling the source noise achieves better performance empirically. (More details are in the next section).</p> <h3 id="the-reconstruction-term-l_0">The Reconstruction Term \(L_0\)</h3> <p>As a bit of a recap, our goal is to simplify the negative variational lower bound (VLB) \(L_{VLB} = L_0 + L_1 + \ldots + L_{T - 1} + L_T\) into an expression we can implement. By the diffusion encoder assumption \((\ref{eq:diff_encoder_assumption})\), we know that the prior matching term \(L_T\) is 0, and the denoising matching terms \(L_{1:T - 1}\) are given by \((\ref{eq:loss_data_recon_weighted})\) (if we are modeling the original data \(x_0\)) or \((\ref{eq:loss_noise_recon})\) (if we are modeling the source noise \(\epsilon_0\)).</p> <p>This leaves us with the reconstruction term \(L_0 = -\mathbb{E}_{x_1 \sim q(x_1 \mid x_0)}[\log{p_\theta(x_0 \mid x_1)}]\) as the final term we haven’t dealt with. Our denoising model suggests that the distribution \(p_\theta(x_0 \mid x_1)\) of the data \(x_0\) given \(x_1\) should be \(\mathcal{N}(x_0; \mu_\theta(x_1, 1), \sigma_1^2)\) (and this is how we modeled \(L_1, \ldots, L_{T - 1}\)).</p> <p>However, in the case of images, each dimension of the image is an integer in \(\{0, 1, \ldots, 255\}\) and we typically scale the data linearly into \([-1, 1]\) before feeding it into our model. So to obtain a discrete (negative) log likelihood for image data, the DDPM authors choose to use an independent discrete decoder based on \(\mathcal{N}(x_0; \mu_\theta(x_1, 1), \sigma_1^2)\). That is, we model \(p_\theta(x_0 \mid x_1)\) as</p> <p>\(\begin{equation} \label{eq:ddpm_recon_decoder} p_\theta(x_0 | x_1) = \prod_{i = 1}^{D}{\int_{\delta_{-}(x_0^i)}^{\delta_{+}(x_0^i)}{\mathcal{N}(x_0; \mu_\theta^i(x_1, 1), \sigma_1^2)dx}} \end{equation}\) where</p> \[\begin{equation*} \delta_{-}(x) = \begin{cases} -\infty &amp; \text{if } x = -1 \\ x - \frac{1}{255} &amp; \text{if } x &gt; -1 \\ \end{cases} \end{equation*}\] \[\begin{equation*} \delta_{+}(x) = \begin{cases} x + \frac{1}{255} &amp; \text{if } x &lt; 1 \\ \infty &amp; \text{if } x = 1 \\ \end{cases} \end{equation*}\] <p>and \(x^i\) refers to the \(i\)th coordinate of \(x\). We could then calculate the (negative) log likelihood of the data under this distribution and then backpropagate through the resulting expression.</p> <p>It turns out that we don’t need to worry about the decoder, since the DDPM authors found that using the following simplified variant of the variational lower bound leads to better sample quality:</p> \[\begin{equation} \label{eq:ddpm_obj_simp} L_{\mbox{simple}}(\theta) = \mathbb{E}_{t \sim U[1, t]}[\mathbb{E}_{\epsilon_0 \sim \mathcal{N}(0, I)}[||\hat{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon_0, t) - \epsilon_0||_2^2]] \end{equation}\] <p>In this objective, the \(t = 1\) case corresponds to the reconstruction loss term in the ELBO. For the \(t &gt; 1\) terms, this can be viewed as an weighted version of the original variational lower bound \((\ref{eq:loss_noise_recon})\). Relative to \(L_{VLB}\), we increase the relative weight of the terms with higher noise levels \(t\), which are more difficult to learn, which the DDPM authors say account for its improved performance. <d-cite key="ho2020denoising"></d-cite> As usual, we can optimize this objective using Monte Carlo samples.</p> <p>In the DDPM paper, the authors found that modeling the original data and modeling the source noise using the original loss functions performed about equally well, but modeling the source noise with the simplified objective \((\ref{eq:ddpm_obj_simp})\) results in a noticeable improvement in generation quality. Interestingly, dropping the time-dependent coefficients from the \(L_t\) terms doesn’t seem to work when modeling the original data. <d-cite key="ho2020denoising"></d-cite></p> <h2 id="summary-of-ddpm-math">Summary of DDPM Math</h2> <p>A variational diffusion model can be thought of as a Markovian hierarchical variational autoencoder (MHVAE) with the following restrictions:</p> <ol> <li> <strong>Diffusion Model Latent Dimension Assumption</strong>: the dimension of the latent variables \(z_{1:T}\) equals the dimension of the data variable \(x\). This allows us to consider the latent variables as representing “noisified” versions \(x_{1:T}\) of the original data \(x_0\).</li> <li> <strong>Diffusion Model Encoder Assumption</strong>: The forward encoders \(q_\phi(x_t \mid x_{t-1})\) are fixed, not learned, and are modeled as a Gaussian centered on the output of the previous encoding step: \(q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t}x_{t-1}, (1 - \alpha_t)I)\). We can think of the noise parameters \(\{\alpha_t\}\) as being hyperparameters of the model; it is possible to extend the model to learn them.</li> <li> <strong>Diffusion Model Prior Assumption</strong>: The noise parameters \(\{\alpha_t\}\) vary such that the distribution \(p(x_T)\) of the final latent variable \(x_T\) is a standard Gaussian. That is, at the end of the forward diffusion process, the data is “totally random”.</li> </ol> <p>With these assumptions, we can derive a variational lower bound of the form \(\log{p(x)} \geq L_0 + L_1 + \ldots L_{t - 1} + \ldots + L_T\) where</p> <p>\(\begin{equation} \label{eq:summary_diff_vlb} \begin{split} L_0 &amp; = \mathbb{E}_{x_1 \sim q(x_1 | x_0)}[\log{p_\theta(x_0 | x_1)}] \\ L_{t - 1} &amp; = -\mathbb{E}_{x_t \sim q(x_t | x_0)}[D_{KL}(q(x_{t - 1} | x_t, x_0) || p_\theta(x_{t - 1} | x_t))] \\ L_T &amp; = -D_{KL}(q(x_T | x_0) || p(x_T)) \\ \end{split} \end{equation}\) Furthermore, we know that the prior matching term \(L_T\) has no learnable parameters via the encoder assumption.</p> <p>Crucially, we can express the data \(x_t\) at noise level \(t\) in closed form as a function of the original data \(x_0\):</p> \[\begin{equation} \label{eq:summary_forward_process_posterior} x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon_0 \end{equation}\] <p>where \(\bar{\alpha}_t = \prod_{i=1}^{t}{\alpha_i}\) and \(\epsilon_0 \sim \mathcal{N}(0, I)\) is a noise variable. (By the reparameterization trick, this is equivalent to saying that \(q(x_t \mid x_0) \sim \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)I)\).)</p> <p>From this and the construction of the encoders in the encoder assumption above, we find that the ground truth denoising transition or forward process posterior \(q(x_{t-1} \mid x_t, x_0)\) is also a Gaussian \(q(x_{t-1} \mid x_t, x_0) \sim \mathcal{N}(x_{t-1}; \mu_q(x_t, x_0), \sigma_q^2(t)I)\) where</p> \[\begin{equation} \label{eq:summary_reverse_ground_truth_posterior_signal} \begin{split} \mu_q(x_t, x_0) &amp; = \frac{\sqrt{\alpha_{t}}(1 - \bar{\alpha}_{t-1})x_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)x_0}{1 - \bar{\alpha}_t} \\ \sigma_q^2(t) &amp; = \frac{(1 - \alpha_t)(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \\ \end{split} \end{equation}\] <p>This suggests that we should model our learned denoising transition \(p_\theta(x_{t-1} \mid x_t)\) as a Gaussian \(\mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_\theta^2(x_t, t)I)\) as well. There are several choices of parameterization we can use for our Gaussian distribution. Since our forward posterior variance \(\sigma_q^2(t)\) depends only on \(t\), a natural choice is to set the variance \(\sigma_\theta^2(x_t, t)\) of our denoising transition to be fixed and equal to \(\sigma_q^2(t)\).</p> <p>For the denoising mean \(\mu_\theta(x_t, t)\), one choice of parameterization we can make is to set</p> \[\begin{equation} \label{eq:summary_orig_data_mean_param} \mu_\theta(x_t, t) = \frac{\sqrt{\alpha_{t}}(1 - \bar{\alpha}_{t-1})x_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)\hat{x}_\theta(x_t, t)}{1 - \bar{\alpha}_t} \end{equation}\] <p>where \(\hat{x}_\theta(x_t, t)\) is a model which learns to reconstruct the original data \(x_0\) from a noisified version \(x_t\) and the time index \(t\). Plugging this into our ELBO, we get a denoising objective \(L_{1:T - 1}\) of the form</p> \[\begin{equation} \label{eq:summary_orig_data_denoising_obj} L_{1:T - 1}(\theta) =\mathbb{E}_{x_t, t}[\frac{1}{2\sigma_q^2(t)}\frac{\bar{\alpha}_{t-1}(1 - \alpha_t)^2}{(1 - \bar{\alpha}_t)^2}||\hat{x}_\theta(x_t, t) - x_0||_2^2] \end{equation}\] <p>where \(x_t \sim q(x_t \mid x_0)\) and \(t \sim \mbox{Uniform}[2, T]\).</p> <p>Alternatively, we can rewrite the ground truth mean in terms of \(x_t\) and the source noise \(\epsilon_0 \sim \mathcal{N}(0, I)\) using \((\ref{eq:summary_forward_process_posterior})\):</p> \[\begin{equation} \label{eq:summary_reverse_ground_truth_posterior_noise} \mu_q(x_t, \epsilon_0) = \frac{1}{\sqrt{\alpha}_t}(x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha_t}}}\epsilon_0) \end{equation}\] <p>This suggests an alternate parameterization</p> \[\begin{equation} \label{eq:summary_source_noise_mean_param} \mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha}_t}(x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha_t}}}\hat{\epsilon}_\theta(x_t, t)) \end{equation}\] <p>where \(\hat{\epsilon}_\theta(x_t, t)\) is a model that learns to reconstruct the source noise \(\epsilon_0\) from a noisified version of the data \(x_t\) and the time index \(t\). This yields a denoising objective of the form</p> \[\begin{equation} \label{eq:summary_source_noise_denoising_obj} L_{1:T - 1}(\theta) = \mathbb{E}_{x_t, t, \epsilon_0}[\frac{1}{2\sigma_q^2(t)}\frac{(1 - \alpha_t)^2}{\alpha_t(1 - \bar{\alpha}_t)}||\hat{\epsilon}_\theta(x_t, t) - \epsilon_0||_2^2] \end{equation}\] <p>where \(x_t \sim q(x_t \mid x_0)\), \(t \sim \mbox{Uniform}[2, T]\), and \(\epsilon_0 \sim \mathcal{N}(0, I)\).</p> <p>We could model the log likelihood term \(L_0(\theta) = \mathbb{E}_{q(x_1 \mid x_0)}[\log{p_\theta(x_0 \mid x_1)}]\) with an explicit decoder, but instead it is common to drop the time-dependent coefficients and use the simplified objective</p> \[\begin{equation} \label{eq:summary_simp_denoising_obj} L_{\mbox{simple}}(\theta) = \mathbb{E}_{t, x_0, \epsilon_0}[||\hat{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon_0, t) - \epsilon_0||_2^2] \end{equation}\] <p>where \(t \sim \mbox{Uniform}[1, T]\) and \(\epsilon_0 \sim \mathcal{N}(0, I)\). The \(t = 1\) case approximates the log likelihood term \(L_0\) in the variational lower bound.</p> <p>For training, we can optimize \((\ref{eq:summary_simp_denoising_obj})\) by approximating the expectation with samples and using a gradient-based optimizer such as Adam. For sampling, we can run the reverse diffusion process one step at a time, repeatedly applying denoising transitions \(p_\theta(x_{t - 1} \mid x_t)\) (using the parameterization we chose in \((\ref{eq:summary_source_noise_mean_param})\)) until we get to the final denoising mean \(\mu_\theta(x_1, 1)\), which we output as our sample.</p> <h2 id="implementing-a-ddpm-model">Implementing a DDPM Model</h2> <p>We need the following ingredients to implement a DDPM-style diffusion model:</p> <ol> <li>A model \(\hat{x}_\theta(x_t, t)\) to model the original data or a model \(\hat{\epsilon}_\theta(x_t, t)\) to model the source noise. Our model needs a way to encode the time index \(t\) as part of its input.</li> <li>A variance schedule to set the variance parameters \(\{\beta_t\} = \{1 - \alpha_t\}\). We want the variances to be monotonically increasing throughout the diffusion process, e.g. \(0 &lt; \beta_1 &lt; \beta_2 &lt; \ldots &lt; \beta_T &lt; 1\).</li> <li>The loss function, which takes samples an arbitrary time index \(t\) and then samples from \(q(x_t \mid x_0)\) and calculates the appropriate loss, as described below.</li> </ol> <p>Following the DDPM paper, we will choose to model the source noise \(\hat{\epsilon}_\theta(x_t, t)\) and use the simplified loss function \(L_{\mbox{simple}}(\theta)\), which were the choices in their best-performing model.</p> <h3 id="model-architecture">Model Architecture</h3> <p>The most popular model architecture for the reverse process model is a U-Net architecture <d-cite key="ronneberger2015u,ho2020denoising,nichol2021improved"></d-cite>, which consists of the following parts:</p> <ol> <li> <strong>Downsampling Blocks</strong>: The downsampling portion of the network consists of blocks which downsamples the image width and height and increases the number of channels. Typically, each block contains some number of residual blocks, followed by a downsample operation.</li> <li> <strong>Bottleneck</strong>: The bottleneck portion of the model performs computation on the input at the lowest image resolution (and highest number of feature channels), which occurs in the middle of the model. This computation typically does not change the shape of the incoming tensor.</li> <li> <strong>Upsampling Blocks</strong>: The upsampling portion of the model is essentially a mirror image of the downsampling portion: each upsampling block upsamples the image width and height and decreases the number of channels. Like the downsample blocks, each upsample block contains a number of residual blocks, followed by an upsample operation. Additionally, each upsampling block has a residual connection to the output of the corresponding downsampling block at the same resolution.</li> </ol> <p>The overall U-Net structure then consists of \(N\) downsample blocks, followed by the bottleneck layer, followed by \(N\) corresponding upsample blocks. Since the upsampling layers reverse the effect of the downsampling layers, we get an output of the same resolution shape as the input, so we can imagine a U-Net as a kind of autoencoder.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ddpm_post/unet_example-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ddpm_post/unet_example-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ddpm_post/unet_example-1400.webp"></source> <img src="/assets/img/ddpm_post/unet_example.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Example U-Net architecture for a 128x128 input image. Image from <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb" rel="external nofollow noopener" target="_blank">the diffusers training example notebook</a>. </div> <p>Additionally, it is common to use an initial and final convolutional layer which do not change the resolution of the representation before the downsampling layers and after the upsampling layers, respectively. <d-cite key="ho2020denoising_impl,Wang2022denoising_impl"></d-cite></p> <h4 id="the-ddpm-u-net-architecture">The DDPM U-Net Architecture</h4> <p>In this section, we will discuss the original DDPM U-Net the authors used for the CIFAR10 dataset, accompanied by a sample implementation in PyTorch. This implementation is based on the original Tensorflow implementation <d-cite key="ho2020denoising_impl"></d-cite> and Phil Wang’s PyTorch implementation <d-cite key="Wang2022denoising_impl"></d-cite>.</p> <h4 id="residual-block">Residual Block</h4> <p>The U-Net in the DDPM paper follows the PixelCNN++ paper <d-cite key="Salimans2017PixeCNN"></d-cite> by using multiple Wide ResNet-style residual blocks <d-cite key="zagoruyko2016wide"></d-cite> in each downsampling and upsampling block. The original Wide ResNet paper used batch normalization and ReLU activations before each convolution, but the DDPM authors instead opt for group normalization <d-cite key="wu2018group"></d-cite> and SiLU activations.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">PreNorm</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""Applies an activation and then group norm before a submodel fn."""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">act_fn</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">Identity</span><span class="p">(),</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fn</span> <span class="o">=</span> <span class="n">fn</span>
        <span class="n">self</span><span class="p">.</span><span class="n">act_fn</span> <span class="o">=</span> <span class="n">act_fn</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GroupNorm</span><span class="p">(</span><span class="n">groups</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">act_fn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResnetBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">act_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">SiLU</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="nc">PreNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="n">act_fn</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="nc">PreNorm</span><span class="p">(</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="n">act_fn</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">res_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">!=</span> <span class="n">dim_out</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Identity</span><span class="p">()</span>

        <span class="c1"># For the time embedding
</span>        <span class="n">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">SiLU</span><span class="p">(),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">time_emb_dim</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">))</span>
            <span class="k">if</span> <span class="nf">exists</span><span class="p">(</span><span class="n">time_emb_dim</span><span class="p">)</span>
            <span class="k">else</span> <span class="bp">None</span>
        <span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">time_emb</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Add the time step embedding, if available.
</span>        <span class="k">if</span> <span class="nf">exists</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">)</span> <span class="ow">and</span> <span class="nf">exists</span><span class="p">(</span><span class="n">time_emb</span><span class="p">):</span>
            <span class="n">time_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp</span><span class="p">(</span><span class="n">time_emb</span><span class="p">)</span>
            <span class="n">time_emb</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">time_emb</span><span class="p">,</span> <span class="s">"b c -&gt; b c 1 1"</span><span class="p">)</span>
            <span class="n">h</span> <span class="o">+=</span> <span class="n">time_emb</span>
        
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">h</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">res_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code></pre></figure> <p>Additionally, the DDPM authors introduce the usage of multi-head attention at the 16x16 resolution, for both downsampling and upsampling. The dot-product attention operation on queries \(Q\) and keys \(K\) of dimension \(d_k\), and values \(V\) is <d-cite key="vaswani2017attention"></d-cite>:</p> \[\begin{equation} \label{dot_prod_attn} \mbox{Attention}(Q, K, V) = \mbox{softmax}(\frac{QK^T}{\sqrt{d_k}})V \end{equation}\] <p>In our case, the input into our attention block is an intermediate U-Net representation (in NCHW format), which we project into different subspaces using a convolutional layer.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""Standard multi-head attention as in a Transformer. We allow all tokens
    to attend to each other, unlike a standard Transformer in NLP."""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">heads</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">dim_head</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>
        <span class="n">self</span><span class="p">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">heads</span>
        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">dim_head</span> <span class="o">*</span> <span class="n">heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">to_qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">to_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">to_qkv</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s">"b (h c) x y -&gt; b h c (x y)"</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">heads</span><span class="p">),</span>
            <span class="n">qkv</span>
        <span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">scale</span>

        <span class="n">sim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="s">"b h d i, b h d j -&gt; b h i j"</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">sim</span> <span class="o">=</span> <span class="n">sim</span> <span class="o">-</span> <span class="n">sim</span><span class="p">.</span><span class="nf">amax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="s">"b h i j, b h d j -&gt; b h i d"</span><span class="p">,</span> <span class="n">attn</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="s">"b h (x y) d -&gt; b (h d) x y"</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">w</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">to_out</span><span class="p">(</span><span class="n">out</span><span class="p">)</span></code></pre></figure> <p>Because the original dot-product attention operation is computationally expensive, we opt to use a linear self-attention variant from <d-cite key="shen2021efficient"></d-cite> in downsampling/upsampling layers.</p> <h4 id="time-embedding">Time Embedding</h4> <p>Since our model \(\epsilon_\theta(x_t, t)\) depends on the time index \(t\), we need some way to incorporate this information into our model. The DDPM authors accomplish this by using a sinusoidal position embedding very similar to the one used in the Transformer model <d-cite key="vaswani2017attention"></d-cite> to encode \(t\), and then add the embeddings to each residual block in both the down and upsampling layers.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">SinusoidalPositionEmbeddings</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">time</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># Assume time is a tensor with shape (batch_size, 1)
</span>        <span class="n">device</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">device</span>
        <span class="n">half_dim</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">half_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">half_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">time</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">embeddings</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">embeddings</span><span class="p">.</span><span class="nf">sin</span><span class="p">(),</span> <span class="n">embeddings</span><span class="p">.</span><span class="nf">cos</span><span class="p">()),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">embeddings</span></code></pre></figure> <h4 id="downsample-layers">Downsample Layers</h4> <p>Each downsample layer of the U-Net then consists of a number of residual blocks, followed by a downsample operation which reduces the spatial dimensions of the representation by 2x:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">Downsample</span><span class="p">(</span><span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
        <span class="nc">Rearrange</span><span class="p">(</span><span class="s">"b c (h p1) (w p2) -&gt; b (c p1 p2) h w"</span><span class="p">,</span> <span class="n">p1</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">p2</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
        <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="nf">default</span><span class="p">(</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="mi">1</span><span class="p">),</span>
    <span class="p">)</span></code></pre></figure> <p>This isn’t exactly the same as the downsample operation used by the original DDPM implementation, but is very similar in that it consists of a convolutional layer which downsamples the resolution by 2x.</p> <p>In addition, each downsample layer also increases the number of channels in the representation by a fixed multiple. The DDPM authors had each downsample layer after the first increase the number of channels by 2x, so that relative to the number of input channels, the number of channels in the downsample representations increases as powers of 2: 1, 2, 4, 8,… This choice has been adopted by most subsequent diffusion U-Net models. <d-cite key="nichol2021improved"></d-cite></p> <h4 id="upsample-layers">Upsample Layers</h4> <p>We can think of each upsample layer as “undoing” the effects of the downsample layer at the same spatial resolution: it increases the spatial extent of the representation and decreases the number of channels of the representation by the corresponding amounts. The upsample layers are laid out in the mirror image of the downsample layers.</p> <p>Each upsample layer consists of the same number of residual blocks as each downsample layer, followed by a upsample operation:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">Upsample</span><span class="p">(</span><span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="p">.</span><span class="nc">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">"nearest"</span><span class="p">),</span>
        <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nf">default</span><span class="p">(</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">)</span></code></pre></figure> <p>Furthermore, each upsample layer has shortcut connections to the downsample layer at the same resolution, with each residual block in the upsample layer connected to the output of the mirror-image residual block in the downsample layer.</p> <h4 id="bottleneck-layer">Bottleneck Layer</h4> <p>For the bottleneck layer of the U-Net, the DDPM authors used a self-attention layer sandwiched by two ResNet blocks, which preserve the spatial dimensions of the representation.</p> <h4 id="summary">Summary</h4> <p>Putting everything together, we have the following U-Net implementation:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">DDPMUnet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">init_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">out_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">dim_mults</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="p">...]</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
        <span class="n">attn_resolutions</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="p">...]</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,),</span>
        <span class="n">image_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">channels</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">resnet_block_groups</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">use_linear_attn</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1"># ----Setup----
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">channels</span> <span class="o">=</span> <span class="n">channels</span>

        <span class="n">init_dim</span> <span class="o">=</span> <span class="nf">default</span><span class="p">(</span><span class="n">init_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>

        <span class="n">dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">init_dim</span><span class="p">,</span> <span class="o">*</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="n">init_dim</span> <span class="o">*</span> <span class="n">m</span><span class="p">,</span> <span class="n">dim_mults</span><span class="p">)]</span>
        <span class="n">resolutions</span> <span class="o">=</span> <span class="p">[</span><span class="n">image_size</span> <span class="o">//</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">dim_mults</span><span class="p">))]</span>
        <span class="n">in_out</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">dims</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">resolutions</span><span class="p">))</span>

        <span class="n">out_dim</span> <span class="o">=</span> <span class="nf">default</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">channels</span><span class="p">)</span>

        <span class="n">attn_cls</span> <span class="o">=</span> <span class="n">Attention</span>
        <span class="k">if</span> <span class="n">use_linear_attn</span><span class="p">:</span>
            <span class="n">attn_cls</span> <span class="o">=</span> <span class="n">LinearAttention</span>
        
        <span class="n">res_block</span> <span class="o">=</span> <span class="nf">partial</span><span class="p">(</span><span class="n">ResnetBlock</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">resnet_block_groups</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="c1"># ----Handle time embeddings----
</span>        <span class="n">time_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="n">self</span><span class="p">.</span><span class="n">time_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">SinusoidalPositionEmbeddings</span><span class="p">(</span><span class="n">dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">time_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">SiLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">time_dim</span><span class="p">,</span> <span class="n">time_dim</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># ----Downsample Layers----
</span>
        <span class="c1"># Initial convolutional layer
</span>        <span class="n">self</span><span class="p">.</span><span class="n">init_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">channels</span><span class="p">,</span> <span class="n">init_dim</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">downsample_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([])</span>
        <span class="n">num_resolutions</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">in_out</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">image_size</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">in_out</span><span class="p">):</span>
            <span class="n">is_last</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">num_resolutions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">use_attn</span> <span class="o">=</span> <span class="n">image_size</span> <span class="ow">in</span> <span class="n">attn_resolutions</span>

            <span class="c1"># Each downsample block consists of:
</span>            <span class="c1">#   - (ResnetBlock + pre-GroupNorm attention block (if using)) x 2
</span>            <span class="c1">#   - 2x Downsample operation x 1
</span>            <span class="n">self</span><span class="p">.</span><span class="n">downsample_layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="nf">res_block</span><span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">),</span>
                        <span class="nc">Residual</span><span class="p">(</span><span class="nc">PreNorm</span><span class="p">(</span><span class="n">dim_out</span><span class="p">,</span> <span class="nf">attn_cls</span><span class="p">(</span><span class="n">dim_out</span><span class="p">)))</span> <span class="k">if</span> <span class="n">use_attn</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Identity</span><span class="p">(),</span>
                        <span class="nf">res_block</span><span class="p">(</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">),</span>
                        <span class="nc">Residual</span><span class="p">(</span><span class="nc">PreNorm</span><span class="p">(</span><span class="n">dim_out</span><span class="p">,</span> <span class="nf">attn_cls</span><span class="p">(</span><span class="n">dim_out</span><span class="p">)))</span> <span class="k">if</span> <span class="n">use_attn</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Identity</span><span class="p">(),</span>
                        <span class="nc">Downsample</span><span class="p">(</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_last</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Identity</span><span class="p">(),</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># ----Bottleneck Layers----
</span>        <span class="n">mid_dim</span> <span class="o">=</span> <span class="n">dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mid_block1</span> <span class="o">=</span> <span class="nf">res_block</span><span class="p">(</span><span class="n">mid_dim</span><span class="p">,</span> <span class="n">mid_dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mid_attn</span> <span class="o">=</span> <span class="nc">Residual</span><span class="p">(</span><span class="nc">PreNorm</span><span class="p">(</span><span class="n">mid_dim</span><span class="p">,</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">mid_dim</span><span class="p">)))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mid_block2</span> <span class="o">=</span> <span class="nf">res_block</span><span class="p">(</span><span class="n">mid_dim</span><span class="p">,</span> <span class="n">mid_dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">)</span>

        <span class="c1"># ----Upsample Layers----
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">upsample_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([])</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">image_size</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">reversed</span><span class="p">(</span><span class="n">in_out</span><span class="p">)):</span>
            <span class="n">is_last</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">num_resolutions</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="n">use_attn</span> <span class="o">=</span> <span class="n">image_size</span> <span class="ow">in</span> <span class="n">attn_resolutions</span>

            <span class="c1"># Each upsample block consists of:
</span>            <span class="c1">#   - (ResnetBlock + pre-GroupNorm attention block (if using)) x 2
</span>            <span class="c1">#   - 2x Upsample operation x 1
</span>            <span class="n">self</span><span class="p">.</span><span class="n">upsample_layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="nf">res_block</span><span class="p">(</span><span class="n">dim_out</span> <span class="o">+</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">),</span>
                        <span class="nc">Residual</span><span class="p">(</span><span class="nc">PreNorm</span><span class="p">(</span><span class="n">dim_out</span><span class="p">,</span> <span class="nf">attn_cls</span><span class="p">(</span><span class="n">dim_out</span><span class="p">)))</span> <span class="k">if</span> <span class="n">use_attn</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Identity</span><span class="p">(),</span>
                        <span class="nf">res_block</span><span class="p">(</span><span class="n">dim_out</span> <span class="o">+</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">dim_in</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">),</span>
                        <span class="nc">Residual</span><span class="p">(</span><span class="nc">PreNorm</span><span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="nf">attn_cls</span><span class="p">(</span><span class="n">dim_in</span><span class="p">)))</span> <span class="k">if</span> <span class="n">use_attn</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Identity</span><span class="p">(),</span>
                        <span class="nc">Upsample</span><span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_in</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_last</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Identity</span><span class="p">()</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">)</span>
        
        <span class="c1"># Final convolutional layer
</span>        <span class="n">self</span><span class="p">.</span><span class="n">final_conv</span> <span class="o">=</span> <span class="nc">PreNorm</span><span class="p">(</span>
            <span class="n">dim</span><span class="p">,</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">act_fn</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="nc">SiLU</span><span class="p">(),</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">resnet_block_groups</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">time</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># x should have initial shape (batch_size, num_channels, height, width)
</span>        <span class="c1"># t should have initial shape (batch_size, 1)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">init_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">time_mlp</span><span class="p">(</span><span class="n">time</span><span class="p">)</span>

        <span class="c1"># Stack of intermediate downsample x values for shortcut connections
</span>        <span class="c1"># between the downsample and upsample layers at the same resolution.
</span>        <span class="n">h</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Downsample
</span>        <span class="k">for</span> <span class="n">block1</span><span class="p">,</span> <span class="n">attn1</span><span class="p">,</span> <span class="n">block2</span><span class="p">,</span> <span class="n">attn2</span><span class="p">,</span> <span class="n">downsample</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">downsample_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">block1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">attn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">h</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

            <span class="n">x</span> <span class="o">=</span> <span class="nf">block2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">attn2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">h</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

            <span class="n">x</span> <span class="o">=</span> <span class="nf">downsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Bottleneck
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mid_block1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mid_attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mid_block2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

        <span class="c1"># Upsample
</span>        <span class="k">for</span> <span class="n">block1</span><span class="p">,</span> <span class="n">attn1</span><span class="p">,</span> <span class="n">block2</span><span class="p">,</span> <span class="n">attn2</span><span class="p">,</span> <span class="n">upsample</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">upsample_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">.</span><span class="nf">pop</span><span class="p">()),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">block1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">attn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">.</span><span class="nf">pop</span><span class="p">()),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">block2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">attn2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

            <span class="n">x</span> <span class="o">=</span> <span class="nf">upsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Final conv layer
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">final_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span></code></pre></figure> <p>Subsequent work has suggested U-Net architecture improvements for diffusion models, e.g. <d-cite key="dhariwal2021diffusion"></d-cite>.</p> <h3 id="variance-schedules">Variance Schedules</h3> <p>In the DDPM paper, the authors used a fixed (as opposed to learned) variance schedule \(0 &lt; \beta_1 &lt; \ldots &lt; \beta_T &lt; 1\). The particular hyperparameter settings they used were a linear variance schedule with \(\beta_1 = 10^{-4}\) and \(\beta_T = 0.02\), with these specific values chosen such that the prior matching term \(L_T \approx 0\) and the variances are small relative to the data scaled to \([-1, 1]\).</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">cosine_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.008</span><span class="p">):</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="n">timesteps</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span>
    <span class="n">alphas_cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(((</span><span class="n">x</span> <span class="o">/</span> <span class="n">timesteps</span><span class="p">)</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">alphas_cumprod</span> <span class="o">=</span> <span class="n">alphas_cumprod</span> <span class="o">/</span> <span class="n">alphas_cumprod</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">betas</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">alphas_cumprod</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">/</span> <span class="n">alphas_cumprod</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.9999</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">linear_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
    <span class="n">beta_start</span> <span class="o">=</span> <span class="mf">0.0001</span>
    <span class="n">beta_end</span> <span class="o">=</span> <span class="mf">0.02</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">beta_start</span><span class="p">,</span> <span class="n">beta_end</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">quadratic_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
    <span class="n">beta_start</span> <span class="o">=</span> <span class="mf">0.0001</span>
    <span class="n">beta_end</span> <span class="o">=</span> <span class="mf">0.02</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">beta_start</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">beta_end</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>


<span class="k">def</span> <span class="nf">sigmoid_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
    <span class="n">beta_start</span> <span class="o">=</span> <span class="mf">0.0001</span>
    <span class="n">beta_end</span> <span class="o">=</span> <span class="mf">0.02</span>
    <span class="n">betas</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">betas</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">beta_end</span> <span class="o">-</span> <span class="n">beta_start</span><span class="p">)</span> <span class="o">+</span> <span class="n">beta_start</span></code></pre></figure> <p>In subsequent work, it was found that the choice of a linear variance schedule could be improved on by using a cosine variance schedule. <d-cite key="nichol2021improved"></d-cite> There are also extensions of the model which allow the variances of the reverse diffusion process to be learned jointly with the mean. <d-cite key="kingma2021variational"></d-cite></p> <h3 id="forward-diffusion-process">Forward Diffusion Process</h3> <p>Since we have a closed-form expression for a noisy image \(x_t\) in terms of the original data \(x_0\) via \((\ref{eq:summary_forward_process_posterior})\), we do not need to implement the forward diffusion process step-by-step. Instead, we can simply sample directly from \(q(x_t \mid x_0) \sim \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)I)\), which is often called the <em>forward process posterior</em>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">DDPM</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="p">...</span>
	<span class="k">def</span> <span class="nf">q_sample</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">x_start</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">noise</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span><span class="o">=</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="s">"""Run the forward diffusion process to get x_start at noise level t.
        
        In particular, this samples from q(x_t | x_0) in batch mode via

        \[ x_t = \sqrt{</span><span class="se">\b</span><span class="s">ar{</span><span class="se">\a</span><span class="s">lpha}_t}x_0 + \sqrt{1 - </span><span class="se">\b</span><span class="s">ar{</span><span class="se">\a</span><span class="s">lpha}_t}\epsilon_0 \]

        where $\epsilon_0$ is a noise sample sampled from a standard Gaussian.
        """</span>
        <span class="k">if</span> <span class="n">noise</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x_start</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">sqrt_alphas_cumprod_t</span> <span class="o">=</span> <span class="nf">extract</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sqrt_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">sqrt_one_minus_alphas_cumprod_t</span> <span class="o">=</span> <span class="nf">extract</span><span class="p">(</span>
            <span class="n">self</span><span class="p">.</span><span class="n">sqrt_one_minus_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_start</span><span class="p">.</span><span class="n">shape</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">sqrt_alphas_cumprod_t</span> <span class="o">*</span> <span class="n">x_start</span> <span class="o">+</span> <span class="n">sqrt_one_minus_alphas_cumprod_t</span> <span class="o">*</span> <span class="n">noise</span>
	<span class="p">...</span></code></pre></figure> <h3 id="training">Training</h3> <p>We can give the following batch training algorithm for a DDPM model modeling the source noise $\epsilon_0$ using the simplified training objective \((\ref{eq:summary_simp_denoising_obj})\):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ddpm_post/ddpm_train_alg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ddpm_post/ddpm_train_alg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ddpm_post/ddpm_train_alg-1400.webp"></source> <img src="/assets/img/ddpm_post/ddpm_train_alg.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Training algorithm for the DDPM model. (Algorithm 1 from <a href="https://arxiv.org/pdf/2006.11239.pdf" rel="external nofollow noopener" target="_blank">the DDPM paper</a>). </div> <p>Essentially, for each data point \(x_0\) we train on, we sample a time step \(t \sim \mbox{Uniform}(\{1, \ldots, T\})\) and some source noise \(\epsilon \sim \mathcal{N}(0, I)\). We then sample from the forward process posterior \(q(x_t \mid x_0) \sim \mathcal{N}(\sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)I)\) using the reparameterization trick and take a gradient step on the loss term associated with \(t\), which is the reconstruction loss \(\vert\vert\epsilon - \hat{\epsilon}_\theta(x_t, t)\vert\vert^2\).</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">train_one_epoch</span><span class="p">(</span>
    <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">ema</span><span class="p">,</span>
    <span class="n">device</span><span class="p">,</span>
    <span class="n">writer</span><span class="p">,</span>
    <span class="n">timesteps</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="n">loss_type</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s">"huber"</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">progress_bar</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)))</span>
    <span class="n">progress_bar</span><span class="p">.</span><span class="nf">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s"> Steps"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Sample timesteps uniformly at random from [0,..., T - 1] each sample
</span>        <span class="c1"># in the batch.
</span>        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>

        <span class="c1"># The model forward pass occurs in the loss function.
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">p_losses</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">loss_type</span><span class="o">=</span><span class="n">loss_type</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

        <span class="c1"># Update the parameters via EMA, if applicable.
</span>        <span class="k">if</span> <span class="nf">exists</span><span class="p">(</span><span class="n">ema</span><span class="p">):</span>
            <span class="n">ema</span><span class="p">.</span><span class="nf">update</span><span class="p">()</span>
        
        <span class="n">step_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
        <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">step_loss</span>
        
        <span class="n">logs</span> <span class="o">=</span> <span class="p">{</span><span class="s">"step_loss"</span><span class="p">:</span> <span class="n">step_loss</span><span class="p">}</span>
        <span class="n">progress_bar</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="o">**</span><span class="n">logs</span><span class="p">)</span>
        <span class="n">progress_bar</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">epoch_loss</span> <span class="o">/=</span> <span class="n">step</span>
    <span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="s">'metrics/training_loss'</span><span class="p">,</span> <span class="n">epoch_loss</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">epoch_loss</span></code></pre></figure> <p>The DDPM authors use <a href="https://en.wikipedia.org/wiki/Exponential_smoothing" rel="external nofollow noopener" target="_blank">exponential moving averaging (EMA)</a> during training, and then use the EMA-averaged model weights for evaluation. Since there is no native PyTorch support for EMA (as of the time of writing and 1.13.0), we use the <a href="https://github.com/fadel/pytorch_ema" rel="external nofollow noopener" target="_blank">torch-ema</a> package; <a href="https://github.com/lucidrains/ema-pytorch" rel="external nofollow noopener" target="_blank">ema-pytorch</a> is another choice.</p> <p>An implementation of the simplified loss \((\ref{eq:summary_simp_denoising_obj})\) is</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">DDPM</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="p">...</span>
	<span class="k">def</span> <span class="nf">p_losses</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">x_start</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">noise</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">loss_type</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s">"l2"</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="s">"""Implements the simplified loss function from the DDPM paper.
        
        This implements equation (14) from the paper:

        \[L(</span><span class="se">\t</span><span class="s">heta) = \mathbb{E}_{t, x_0, \epsilon}[||\epsilon - \epsilon_</span><span class="se">\t</span><span class="s">heta(x_t, t)||_2^2\]

        via a single-sample estimate for a batch of x_t and t.
        """</span>
        <span class="k">if</span> <span class="n">noise</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x_start</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># Get x_t samples from q(x_t | x_0)
</span>        <span class="n">x_noisy</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">q_sample</span><span class="p">(</span><span class="n">x_start</span><span class="o">=</span><span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Get noise predictions from our denoising model $\epsilon_\theta(x_t, t)$.
</span>        <span class="n">predicted_noise</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">denoise_model</span><span class="p">(</span><span class="n">x_noisy</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

        <span class="c1"># L_simple uses the L2 loss, but we implement the ability to experiment
</span>        <span class="c1"># with other reconstruction losses.
</span>        <span class="k">if</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="s">'l1'</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">l1_loss</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">predicted_noise</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="s">'l2'</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">predicted_noise</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="s">'huber'</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">smooth_l1_loss</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">predicted_noise</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s">"Loss type </span><span class="si">{</span><span class="n">loss_type</span><span class="si">}</span><span class="s"> is not implemented."</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">loss</span>
	<span class="p">...</span></code></pre></figure> <h3 id="sampling">Sampling</h3> <p>We now know how to train a diffusion model. After training a model, how can we sample from it?</p> <p>As with other generative models, we start with a noise sample \(x_T \sim \mathcal{N}(x_T; 0, I)\) from our prior \(p(x_T)\). We then run the reverse diffusion process one step at a time:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">DDPM</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="p">...</span>
	<span class="nd">@torch.no_grad</span><span class="p">()</span>
	<span class="k">def</span> <span class="nf">p_sample_loop</span><span class="p">(</span>
		<span class="n">self</span><span class="p">,</span>
		<span class="n">shape</span><span class="p">,</span>
		<span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span><span class="o">=</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">,</span>
	<span class="p">):</span>
		<span class="s">"""Implements the sampling loop for the reverse diffusion process.

		This runs the reverse diffusion process from timestep T - 1 down to 0,
		getting samples at timestep t using the p_sample() method.
		"""</span>
		<span class="n">b</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
		<span class="c1"># Start from pure noise for each sample in the batch.
</span>		<span class="n">img</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
		<span class="n">imgs</span> <span class="o">=</span> <span class="p">[]</span>

		<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span>
			<span class="nf">reversed</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">timesteps</span><span class="p">)),</span>
			<span class="n">desc</span><span class="o">=</span><span class="s">'sampling loop time step'</span><span class="p">,</span>
			<span class="n">total</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">timesteps</span>
			<span class="p">):</span>
			<span class="n">img</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">p_sample</span><span class="p">(</span>
				<span class="n">img</span><span class="p">,</span>
				<span class="n">torch</span><span class="p">.</span><span class="nf">full</span><span class="p">((</span><span class="n">b</span><span class="p">,),</span> <span class="n">i</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">),</span>
				<span class="n">i</span>
			<span class="p">)</span>
			<span class="n">imgs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="nf">cpu</span><span class="p">())</span>
		
		<span class="k">return</span> <span class="n">imgs</span>

	<span class="nd">@torch.no_grad</span><span class="p">()</span>
	<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
		<span class="n">self</span><span class="p">,</span>
		<span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
		<span class="n">channels</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
		<span class="n">image_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">28</span><span class="p">,</span>
		<span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span><span class="o">=</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">,</span>
	<span class="p">):</span>
		<span class="s">"""Sample from the diffusion model by running the reverse diffusion process."""</span>
		<span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">p_sample_loop</span><span class="p">(</span>
			<span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">image_size</span><span class="p">,</span> <span class="n">image_size</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
		<span class="p">)</span>
	<span class="p">...</span></code></pre></figure> <p>By construction, we know that \(p_\theta(x_{t - 1} \mid x_t)\) has distribution \(\mathcal{N}(\frac{1}{\sqrt{\alpha_t}}(x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}}\hat{\epsilon}_\theta(x_t, t)), \sigma_q^2(t)I)\) (from \((\ref{eq:summary_source_noise_mean_param})\)). Using the reparameterization trick, we can first sample some noise \(z \sim \mathcal{N}(0, I)\), and then sample \(x_{t - 1}\) according to</p> \[x_{t - 1} = \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}}\hat{\epsilon}_\theta(x_t, t)) + \sigma_q(t)z\] <p>At the last denoising step, we output the denoising mean \(x_0 = \mu_\theta(x_1, 1)\) without any noise.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">DDPM</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="p">...</span>
	<span class="nd">@torch.no_grad</span><span class="p">()</span>
	<span class="k">def</span> <span class="nf">p_sample</span><span class="p">(</span>
		<span class="n">self</span><span class="p">,</span>
		<span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
		<span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
		<span class="n">t_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
	<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
		<span class="s">"""Run one step of the reverse diffusion process.
		
		This implements one step in the loop in Algorithm 2 of the DDPM paper
		in batch modee. Given the previous sample x_t at noise level t, we
		get a sample x_{t-1} at noise level t - 1 using our formula for the
		denoising transition $p_</span><span class="se">\t</span><span class="s">heta(x_{t-1} | x_t)$ via the
		reparameterization trick.
		"""</span>
		<span class="n">betas_t</span> <span class="o">=</span> <span class="nf">extract</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">betas</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

		<span class="n">sqrt_one_minus_alphas_cumprod_t</span> <span class="o">=</span> <span class="nf">extract</span><span class="p">(</span>
			<span class="n">self</span><span class="p">.</span><span class="n">sqrt_one_minus_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
		<span class="p">)</span>
		<span class="n">sqrt_recip_alphas_t</span> <span class="o">=</span> <span class="nf">extract</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sqrt_recip_alphas</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

		<span class="c1"># Use our noise prediction model to predict the mean
</span>		<span class="n">model_mean</span> <span class="o">=</span> <span class="n">sqrt_recip_alphas_t</span> <span class="o">*</span> <span class="p">(</span>
			<span class="n">x</span> <span class="o">-</span> <span class="n">betas_t</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">denoise_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt_one_minus_alphas_cumprod_t</span>
		<span class="p">)</span>

		<span class="k">if</span> <span class="n">t_index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
			<span class="k">return</span> <span class="n">model_mean</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">posterior_variance_t</span> <span class="o">=</span> <span class="nf">extract</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">posterior_variance</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
			<span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
			<span class="k">return</span> <span class="n">model_mean</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">posterior_variance_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise</span>
	<span class="p">...</span></code></pre></figure> <p>This algorithm is summarized in the following algorithm:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ddpm_post/ddpm_sampling_alg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ddpm_post/ddpm_sampling_alg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ddpm_post/ddpm_sampling_alg-1400.webp"></source> <img src="/assets/img/ddpm_post/ddpm_sampling_alg.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Sampling algorithm for the DDPM model. (Algorithm 2 from the <a href="https://arxiv.org/pdf/2006.11239.pdf" rel="external nofollow noopener" target="_blank">the DDPM paper</a>). </div> <h3 id="metrics-and-evaluation">Metrics and Evaluation</h3> <p>In this section, we will discuss several metrics used for quantitatively evaluating image generation models (which are not specific to diffusion models), with an emphasis on metrics used in the original DDPM paper. The current standard metric is Fréchet Inception Distance (FID), which corresponds well with human judgments; however, it is not considered a “gold standard” and finding better sample quality metrics is still an open research question. <d-cite key="dhariwal2021diffusion"></d-cite></p> <h4 id="likelihood-and-bits-per-dimension">Likelihood and Bits Per Dimension</h4> <p>A natural metric to use is the likelihood of the model. To make results comparable across different models and datasets, this is typically measured in <em>bits per dimension (BPD)</em>. For images, this quantity is sometimes called <em>bits per pixel</em>, where each color channel counts as a “pixel”. <d-cite key="papamakarios2017masked"></d-cite> The formula for bits per dimension \(b(x)\) is as follows:</p> \[\begin{equation} \label{eq:bits_per_dim} b(x) = -\frac{\log{p(x)}}{D\log{2}} \end{equation}\] <p>Intuitively, we can calculate the bits per dimension by estimating the negative log likelihood, dividing by the number of dimensions \(D\), and then converting to bits (hence to \(\log{2}\) factor). <d-footnote>Here all logarithms are the natural logarithm.</d-footnote></p> <p>For a diffusion model, we can upper bound the negative log likelihood with the negative variational lower bound \((\ref{eq:summary_diff_vlb})\).</p> <p>Bits per dimension is often reported on the test set. It can also be used as a measure of whether a generative model is overfitting; for a well-trained model, we expect the training BPD and test BPD to be very similar. <d-cite key="ho2020denoising"></d-cite></p> <p>However, using likelihood or bits/dim as a metric has drawbacks. The likelihood can be dominated by single samples, and is often not indicative of the visual sample quality of the model. <d-cite key="theis2015note"></d-cite></p> <h4 id="inception-score">Inception Score</h4> <p>The <em>Inception Score (IS)</em> metric was introduced in <d-cite key="salimans2016improved"></d-cite>. The idea is as follows: suppose we are training on a labeled dataset and have a strong classifier model \(p(y \mid x)\) on that datset. If we generate a batch of images \(x_{gen}\) from a well-trained generative model, we should expect the following to be true:</p> <ol> <li>The objects in the generated images should be distinctive, so \(p(y \mid x_{gen})\) should have low entropy: we expect the distribution to be sharply peaked at the label of the object which we generated an image of.</li> <li>The model should generate a variety of objects in its images, so the marginal distribution of labels \(p(y) = \int{p(y \mid x_{gen}) dx_{gen}}\) should have high entropy.</li> </ol> <p>Thus, for a good generative model \(G(x)\), we would expect the distributions \(p(y \mid x)\) and \(p(y)\) to look very different for \(x \sim G(x)\), so the KL divergence between them \(D_{KL}(p(y \mid x) \vert\vert p(y))\) should be large. So we define the Inception Score metric as follows:</p> \[\begin{equation} \label{eq:inception_score} IS = \exp(\mathbb{E}_{x \sim G(x)}[D_{KL}(p(y | x) || p(y))]) \end{equation}\] <p>We exponentiate the KL divergence to make the scores easier to compare. Better generative models should have higher Inception scores; intuitively, either increasing the sample quality of a generative model or the diversity of its generated images should increase the IS.</p> <p>The paper introducing IS found that it corresponded well with human judgments. However, a drawback of the IS score is that it never references real images directly <d-cite key="heusel2017gans"></d-cite>. Furthermore, IS does not reward covering the whole distribution of images in the dataset and cannot capture intra-class diversity (e.g. generating different breeds of dogs which are all labeled “dog” in the dataset). A final weakness of IS is that models can achieve a high IS simply by memorizing a small subset of the data. <d-cite key="dhariwal2021diffusion"></d-cite></p> <p>In practice, the classifier of choice for calculating IS is the Inception V3 model (hence the name). For maximum comparability, it is recommended to use the reference implementation <d-cite key="inception_score_ref_impl"></d-cite>, since differences in implementation and backends can affect the scores. Typically, a sample of 50k generated images is used to compute the IS.</p> <h4 id="fréchet-inception-distance">Fréchet Inception Distance</h4> <p>The <em>Fréchet Inception Distance (FID)</em> metric was introduced in <d-cite key="heusel2017gans"></d-cite> to address some shortcomings in the Inception score, particularly the fact that it does not compare generated images to real images.</p> <p>As with the Inception Score, suppose we are training on a labeled dataset and have a strong classifier \(p(y \mid x)\) for that dataset. If we have a batch of real images \(x_{real} \sim D\) and a batch of generated images \(x_{gen} \sim G(x)\) for a generative model \(G(x)\), then the more similar the visual features (the latent representation of the classifier before the softmax layer) computed by the classifier on \(x_{gen}\) are to the visual features computed on \(x_{real}\), the better we would expect the sample quality to be.</p> <p>To make the notion of “similar” more precise, suppose \(X_{real}\) is a random variable holding the value of the classifier visual feature tensor for the real images \(x_{real}\), and \(X_{gen}\) is defined similarly. In general, \(X_{real}\) and \(X_{gen}\) might have a very complicated distribution, but we can approximate this distribution using the first and second <a href="https://en.wikipedia.org/wiki/Moment_(mathematics)" rel="external nofollow noopener" target="_blank">moments</a> or <a href="https://en.wikipedia.org/wiki/Cumulant" rel="external nofollow noopener" target="_blank">cumulants</a>, which are the mean and covariance respectively. In particular, we will assume that \(X_{real}\) is distributed according to a multivariate Gaussian with mean \(\mu_{real}\) and covariance \(C_{real}\), and analogously \(X_{gen} \sim \mathcal{N}(\mu_{gen}, C_{gen})\). Then the Fréchet Inception Distance is defined as the <a href="https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance" rel="external nofollow noopener" target="_blank">Fréchet distance</a> between the two Gaussian distributions:</p> \[\begin{equation} \label{eq:frechet_inception_distance} d^2(X_{real}, X_{gen}) = ||\mu_{real} - \mu_{gen}||_2^2 + \mbox{tr}(C_{real} + C_{gen} + 2\sqrt{C_{real}C_{gen}}) \end{equation}\] <p>Unlike the IS, better FID scores are lower scores, because it is a measure of distance between distributions. Like the IS, FID correlates well with human judgments.</p> <p>The FID is considered the standard metric for evaluating generated sample quality as the time of writing. <d-cite key="dhariwal2021diffusion"></d-cite> However, as noted above, it is not considered the “gold standard”, and there is ongoing research into developing better sample quality metrics.</p> <p>In practice, the FID score uses the Inception V3 model as its classifier. The most common choice for the visual features is the output of the final pooling layer (typically called “pool3”) before the classification layer, although other choices are possible. <d-cite key="Seitzer2020FID"></d-cite>. The reference implementation is in Tensorflow <d-cite key="fid_pytorch_impl"></d-cite>, and there also exist implementations for PyTorch <d-cite key="Seitzer2020FID"></d-cite>. The FID scores given by different implementation can vary, so for comparability, it is recommended to use the official reference implementation.</p> <p>Because the dimension of the Inception V3 pool3 layer is 2048, using at least 2048 samples is required to avoid numerical issues. It is common to use at least 10k samples; for example, on the CIFAR-10 dataset, it is common to report the FID on the training set (50k samples) and test set (10k samples). <d-cite key="ho2020denoising"></d-cite>.</p> <h2 id="further-reading">Further Reading</h2> <p>The DDPM model can be thought of as the “ancestor” of all modern diffusion models. Here is a (necessarily incomplete) list of papers which build upon the DDPM model:</p> <ol> <li> <strong>Model Extensions</strong> <ol> <li> <strong><a href="https://arxiv.org/pdf/2010.02502.pdf" rel="external nofollow noopener" target="_blank">Denoising Diffusion Implicit Models (Song, Meng, and Ermon 2021)</a></strong>: generalizes the DDPM model to a non-Markovian models with the same objective as the DDPM model. This generalization also leads an improved, deterministic sampling process which is faster than the original DDPM sampling process (often called “DDIM sampling”).</li> <li> <strong><a href="https://arxiv.org/abs/2102.09672" rel="external nofollow noopener" target="_blank">Improved denoising diffusion probabilistic models (Nichol and Dhariwal 2021)</a></strong>: suggests various improvements to the model, such as a method to learn the variances, a new loss function to improve the likelihood, and a faster method of sampling from the model.</li> <li> <strong><a href="https://arxiv.org/abs/2105.05233" rel="external nofollow noopener" target="_blank">Diffusion models beat gans on image synthesis (Dhariwal and Nichol 2021)</a></strong>: suggests U-Net architecture improvements based on an ablation analysis and introduces classifier guidance.</li> <li> <strong><a href="https://arxiv.org/pdf/2106.15282.pdf" rel="external nofollow noopener" target="_blank">Cascaded Diffusion Models for High Fidelity Image Generation (Ho et al. 2021)</a></strong>: gives a method for training cascaded diffusion models, which are diffusion models connected in series, with each subsequent model at operating at higher and higher resolutions.</li> <li> <strong><a href="https://arxiv.org/pdf/2107.03006.pdf" rel="external nofollow noopener" target="_blank">Structured Denoising Diffusion Models in Discrete State-Spaces (Austin et al. 2021)</a></strong>: explores extensions of diffusion models for discrete (rather than continuous) data, introducing the D3PM model.</li> <li> <strong><a href="https://arxiv.org/abs/2107.00630" rel="external nofollow noopener" target="_blank">Variational diffusion models (Kingma et al. 2021)</a></strong>: gives an extension of the diffusion model to continuous time based on modeling the signal-to-noise ratio of the diffusion process. This also provides another method of learning the variances of the model.</li> <li> <strong><a href="https://arxiv.org/pdf/2112.10752.pdf" rel="external nofollow noopener" target="_blank">High-Resolution Image Synthesis with Latent Diffusion Models (Rombach et al. 2021)</a></strong>: introduces Latent Diffusion Models (LDM), whose key idea is the following: instead of having the diffusion model operate directly in pixel space, we can instead have it operate in the latent space learned by an encoder-decoder model (a VQGAN in the paper). This is the basis of the popular Stable Diffusion model.</li> </ol> </li> <li> <strong>Guidance</strong> <ol> <li> <strong><a href="https://arxiv.org/abs/2105.05233" rel="external nofollow noopener" target="_blank">Diffusion models beat gans on image synthesis (Dhariwal and Nichol 2021)</a></strong>: introduces classifier guidance (duplicated from above), where we can train a classifier on noisy images and use its predictions to guide the samples of the diffusion model.</li> <li> <strong><a href="https://openreview.net/pdf?id=qw8AKxfYbI" rel="external nofollow noopener" target="_blank">Classifier-Free Diffusion Guidance (Ho and Salimans 2021)</a></strong>: introduces classifier-free guidance, in which we train a conditional diffusion model on both conditional and unconditional examples, removing the need for a separate classifier.</li> </ol> </li> <li> <strong>Applications</strong> <ol> <li> <strong><a href="https://arxiv.org/pdf/2112.10752.pdf" rel="external nofollow noopener" target="_blank">High-Resolution Image Synthesis with Latent Diffusion Models (Rombach et al. 2021)</a></strong>: the basis for the Stable Diffusion model (duplicated from above).</li> <li> <strong><a href="https://arxiv.org/pdf/2112.10741.pdf" rel="external nofollow noopener" target="_blank">GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models (Nichol et al. 2021)</a></strong>: develops a text-to-image diffusion model using classifier-free guidance and a noise-aware CLIP model.</li> <li> <strong><a href="https://arxiv.org/pdf/2204.06125.pdf" rel="external nofollow noopener" target="_blank">Hierarchical Text-Conditional Image Generation with CLIP Latents (Ramesh et al. 2022)</a></strong>: introduces the DALL-E 2 model.</li> <li> <strong><a href="https://arxiv.org/pdf/2205.11487.pdf" rel="external nofollow noopener" target="_blank">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (Saharia et al. 2022)</a></strong>: introduces the Imagen model.</li> </ol> </li> <li> <strong>Miscellaneous</strong> <ol> <li> <strong><a href="https://arxiv.org/pdf/2208.09392.pdf" rel="external nofollow noopener" target="_blank">Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise (Bansal et al. 2022)</a></strong>: explores diffusion-like models which replace Gaussian noise noisification process with arbitrary image transformations, including deterministic transformations. The authors find that training a network to invert these transformations also gives rise to generative models similar to DDPM.</li> <li> <strong><a href="https://arxiv.org/pdf/2301.00704.pdf" rel="external nofollow noopener" target="_blank">Muse: Text-To-Image Generation via Masked Generative Transformers (Chang et al. 2023)</a></strong>: a non-diffusion model which achieves SOTA performance for image generation. Like a LDM model, Muse operates in the latent space of a VQGAN, but uses a transformer with a masked token objective rather than a diffusion model.</li> </ol> </li> </ol> <p>I hope to cover many of these papers in subsequent posts.</p> <h2 id="changelist">Changelist</h2> <ul> <li>2023-02-26: Initial post</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2023-01-13-ddpm.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Daniel Gu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>