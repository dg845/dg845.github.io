---
layout: distill
title: The DDPM Model
description: Notes on the DDPM diffusion generative model.
giscus_comments: true
date: 2023-01-13

authors:
  - name: Daniel Gu

bibliography: 2023-01-13-ddpm.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Introduction
  - name: Hierarchical Variational Autoencoders
  - name: Markovian Hierarchical Variational Autoencoders
    subsections:
      - name: Deriving an ELBO
      - name: Deriving a Lower Variance ELBO
  - name: Denoising Diffusion Probabilistic Models
    subsections:
      - name: Simplifying the ELBO
      - name: Learning Noise instead of Signal
      - name: Wait, what about the Reconstruction Term?
  - name: Summary of DDPM Math
  - name: Implementing a DDPM Model
    subsections:
      - name: Model Architecture
      - name: Variance Schedules
      - name: Training
      - name: Sampling
      - name: Evaluation and Metrics
  - name: Practical Notes on Training a Diffusion Model

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
# _styles: >
#   .fake-img {
#     background: #bbb;
#     border: 1px solid rgba(0, 0, 0, 0.1);
#     box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
#     margin-bottom: 12px;
#   }
#   .fake-img p {
#     font-family: monospace;
#     color: white;
#     text-align: left;
#     margin: 12px 0;
#     text-align: center;
#     font-size: 16px;
#   }

---

$$
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$

## Introduction

TODO

## Hierarchical Variational Autoencoders

In a standard variational autoencoder (VAE) model, we have a set of data variables $$x$$ and latent variables $$z$$, and we are interested in modeling the marginal distribution of the data $$p(x)$$. This is done through learning an encoder $$q_\phi(z \mid x)$$ which is a variational approximation to the true posterior $$p(z \mid x)$$ and a decoder $$p_\theta(x \mid z)$$ which recovers the data distribution conditioned on the latent variables $$z$$. This model is typically trained by maximizing a variational lower bound (the ELBO):

$$
\begin{equation}
\label{eq:vae_elbo}
\log{p(x)} \geq \mathbb{E}_{z \sim q_\phi(z | x)}[\log{p_\theta(x | z)}] - D_{KL}(q_\phi(z | x) || p(z))
\end{equation}
$$

A *hierarchical variational autoencoder (HVAE)* extends this model by introducing higher level latents $$z_1, z_2, \ldots, z_T$$, where a higher level latent variable $$z_t$$ is allowed to condition on all lower level latents $$z_1, \ldots, z_{t - 1}$$. As with a VAE, maximizing the log likelihood $$\log{\int{p(x, z_{1:T})dz_{1:T}}}$$ of this model is generally intractable, but we can still derive an ELBO using a variational approximation $$q_\phi(z_{1:T} \mid x)$$:

$$
\begin{equation}
\label{eq:hvae_elbo}
	\begin{split}
		\log{p(x)} & = \log{\int{p(x, z_{1:T})dz_{1:T}}} \\
		& = \log{\int{\frac{p(x, z_{1:T})q_\phi(z_{1:T} | x)}{q_\phi(z_{1:T} | z)}dz_{1:T}}} \\
		& = \log{\mathbb{E}_{z_{1:T} \sim q_\phi(z_{1:T} | x)}[\frac{p(x, z_{1:T})}{q_\phi(z_{1:T} | x)}]} \\
		& \geq \mathbb{E}_{z_{1:T} \sim q_\phi(z_{1:T} | x)}[\log{\frac{p(x, z_{1:T})}{q_\phi(z_{1:T} | x)}}]
	\end{split}
\end{equation}
$$

The last line follows by [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality) for a concave function $$f$$: $$f(\mathbb{E}[x]) \geq \mathbb{E}[f(x)]$$.

To simplify $$(\ref{eq:hvae_elbo})$$ further, we need an expression for the joint distribution $$p(x, z_{1:T})$$ and the posterior $$q_\phi(z_{1:T} \mid x)$$.

## Markovian Hierarchical Variational Autoencoders

We will now make the following simplifying assumption: the latent variables $$z$$ satisfy the Markov property as we move from higher-level latents to lower level latents. That is, we can express the decoding transition from $$z_{t+1}$$ to $z_t$ as $$p(z_t \mid z_{t + 1})$$. The generative process (starting from $$z_T$$, whose distribution $$p(z_T)$$ we specify by construction) is then a Markov chain, and we call a HVAE satisfying this property a *Markovian hierarchical variational autoencoder (MHVAE)*. This has a nice visual representation as follows: [TODO: add picture of MHVAE]

Using the Markov property, we can write down the joint distribution of a MHVAE explicitly as

$$
\begin{equation}
\label{eq:mhvae_joint_dist}
	p_\theta(x, z_{1:T}) = p_\theta(x | z_1)\prod_{t = 2}^{T}{p_\theta(z_{t - 1} | z_t)} \cdot p(z_T)
\end{equation}
$$
Similarly, we can write the posterior distribution of the latent variables as

$$
\begin{equation}
\label{eq:mhvae_post}
	q_\phi(z_{1:T} | x) = q_\phi(z_1 | x)\prod_{t = 2}^{T}{q_\phi(z_t | z_{t - 1})}
\end{equation}
$$
To make the math simpler, we will make the following assumption:

**Diffusion Model Latent Dimension Assumption.** The dimension of the latent variables $$z_{1:T}$$ equals the dimension of the data variable $$x$$.

We will thus rename the data variable to $$x_0$$ and the latent variables to $$x_{1:T}$$. With our new notation, we can rewrite the joint distribution as follows

$$
\begin{equation}
\label{eq:reverse_diff_process}
	p_\theta(x_{0:T}) = p(x_T)\prod_{t = 1}^{T}{p_\theta(x_{t - 1} | x_t)}
\end{equation}
$$
This Markov chain defines the *reverse diffusion process*, starting from the prior $$p(x_T)$$. Similarly, for the posterior, we have

$$
\begin{equation}
\label{eq:forward_diff_process}
	q_\phi(x_{1:T} | x_0) = \prod_{t = 1}^{T}{q_\phi(x_t | x_{t - 1})}
\end{equation}
$$
This is also a Markov chain and defines the *forward diffusion process*. We will see how to interpret these terms later.

### Deriving an ELBO

Now we can simplify our ELBO further:

$$
\begin{equation}
\label{eq:mhvae_elbo1_deriv}
	\begin{split}
		\log{p(x)} & \geq \mathbb{E}_{x_{1:T} \sim q_\phi(x_{1:T} | x_0)}[\log\frac{p_\theta(x_{0:T})}{q_\phi(x_{1:T} | x_0)}] \\
		& = \mathbb{E}_{x_{1:T} \sim q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)\prod_{t = 1}^{T}{p_\theta(x_{t - 1} | x_t)}}{\prod_{t = 1}^{T}{q_\phi(x_t | x_{t - 1})}}] \\
		& = \mathbb{E}_{x_{1:T} \sim q_\phi(x_{1:T} | x_0)}[\log\frac{p_\theta(x_0 | x_1)\prod_{t = 2}^{T}{p_\theta(x_{t - 1} | x_t)} \cdot p(x_T)}{\prod_{t = 1}^{T - 1}{q_\phi(x_t |x_{t - 1})} \cdot q_\phi(x_T | x_{T - 1})}] \\
		& = \mathbb{E}_{x_{1:T} \sim q_\phi(x_{1:T} | x_0)}[\log\frac{p_\theta(x_0 | x_1)\prod_{t = 1}^{T - 1}{p_\theta(x_{t} | x_{t + 1})} \cdot p(x_T)}{\prod_{t = 1}^{T - 1}{q_\phi(x_t |x_{t - 1})} \cdot q_\phi(x_T | x_{T - 1})}] \\
		& = \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log{p_\theta(x_0 | x_1)}] + \sum_{t = 1}^{T - 1}{\mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p_\theta(x_t | x_{t + 1})}{q_\phi(x_t | x_{t - 1})}]} + \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)}{q_\phi(x_T | x_{T - 1})}] \\
		& = \mathbb{E}_{q_\phi(x_1 | x_0)}[\log{p_\theta(x_0 | x_1)}] + \sum_{t = 1}^{T - 1}{\mathbb{E}_{q_\phi(x_{t - 1}, x_t, x_{t + 1} | x_0)}[\log\frac{p_\theta(x_t | x_{t + 1})}{q_\phi(x_t | x_{t - 1})}]} + \mathbb{E}_{q_\phi(x_{T - 1}, x_T | x_0)}[\log\frac{p(x_T)}{q_\phi(x_T | x_{T - 1})}] \\
		& = \mathbb{E}_{q_\phi(x_1 | x_0)}[\log{p_\theta(x_0 | x_1)}] - \sum_{t = 1}^{T - 1}{\mathbb{E}_{q_\phi(x_{t - 1}, x_t, x_{t + 1} | x_0)}[\log\frac{q_\phi(x_t | x_{t - 1})}{p_\theta(x_t | x_{t + 1})}]} - \mathbb{E}_{q_\phi(x_{T - 1}, x_T | x_0)}[\log\frac{q_\phi(x_T | x_{T - 1})}{p(x_T)}] \\
	\end{split}
\end{equation}
$$

Since $$q_\phi(x_{t - 1}, x_t, x_{t + 1} \mid x_0) = q_\phi(x_t \mid x_{t -1}, x_{t + 1}, x_0)q_\phi(x_{t - 1}, x_{t + 1} \mid x_0)$$ by the chain rule, which we can simplify to $$q_\phi(x_t \mid x_{t - 1})q_\phi(x_{t - 1}, x_{t + 1} \mid x_0)$$ by the Markov property, and similarly $$q_\phi(x_{T - 1}, x_T \mid x_0) = q_\phi(x_{T - 1} \mid x_0)q_\phi(x_T \mid x_{T - 1})$$, we can rewrite the second and third terms as expectations over KL divergences as follows:

$$
\begin{equation}
\label{eq:vdm_elbo1}
	\begin{split}
		\log{p(x_0)} \geq & \mathbb{E}_{q_\phi(x_1 | x_0)}[\log{p_\theta(x_0 | x_1)}] \\
		& - \sum_{t=1}^{T-1}{\mathbb{E}_{q_\phi(x_{t-1}, x_{t+1} | x_0)}[D_{KL}(q_\phi(x_t | x_{t-1}) || p_\theta(x_t | x_{t+1}))]} \\
		& - \mathbb{E}_{q_\phi(x_{T-1} | x_0)}[D_{KL}(q_\phi(x_T | x_{T-1}) || p(x_T))]
	\end{split}
\end{equation}
$$

Thus, the ELBO can be written as a sum of terms $$L_0 + L_1 + \ldots + L_t + \ldots + L_T$$ which can be interpreted as follows:

1. **Reconstruction term** $$L_0 = \mathbb{E}_{q_\phi(x_1 \mid x_0)}[\log{p_\theta(x_0 \mid x_1)}]$$: this term expresses how well we are able to recover the original data $$x$$ from the first level latent $$x_1$$, and is the direct analogue of the reconstruction term in the VAE ELBO.
2. **Consistency terms** $$L_t = \mathbb{E}_{q_\phi(x_{t-1}, x_{t+1} \mid x_0)}[D_{KL}(q_\phi(z_t \mid z_{t-1}) \vert\vert p_\theta(z_t \mid z_{t+1}))]$$: these terms capture how close the noising transition from $$x_{t - 1}$$ to $$x_t$$ matches the denoising transition from $$x_{t + 1}$$ to $$x_t$$. Ideally, we want these to have the same distribution: we should end up with data at the same noise level whether we are going forwards or backwards.
3. **Prior Matching term** $$L_T = \mathbb{E}_{q_\phi(x_{T - 1} \mid x_0)}[D_{KL}(q_\phi(x_T \mid x_{T-1}) \vert\vert p(x_T))]$$: this captures how well our last encoding transition $$q_\phi(x_T \mid x_{T - 1})$$ matches the latent prior $$p(x_T)$$, and is the direct analogue of the prior matching term in the VAE ELBO.

The ELBO we derived also has a nice visual representation: [TODO: add picture of MHVAE ELBO 1].

We could try to directly optimize this ELBO, but a problem with it is that the consistency terms are expectations over two random variables $$x_{t-1}$$ and $$x_{t+1}$$. Since we would normally try to estimate these expectations using Monte Carlo samples, we may have to take a large number of samples to reduce the variance of these terms to an acceptable level.

### Deriving a Lower Variance ELBO

Let us try to rewrite the ELBO such that all of our expectations are over only one random variable. We can begin the derivation as follows:

$$
\begin{equation}
\label{eq:vdm_elbo2_deriv1}
	\begin{split}
		\log{p(x)} & \geq \mathbb{E}_{x_{1:T} \sim q(x_{1:T} | x_0)}[\log\frac{p_\theta(x_{0:T})}{q_\phi(x_{1:T} | x_0)}] \\
		& = \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)\prod_{t = 1}^{T}{p_\theta(x_{t-1} | x_t)}}{\prod_{t = 1}^{T}{q_\phi(x_t | x_{t-1})}}] \\
		& = \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)p_\theta(x_0 | x_1)\prod_{t = 2}^{T}{p_\theta(x_{t-1} | x_t)}}{q_\phi(x_1 | x_0)\prod_{t = 2}^{T}{q_\phi(x_t | x_{t-1})}}]
	\end{split}
\end{equation}
$$
Here we can make a key insight: due to the Markov property, we can write

$$
\begin{equation}
\label{eq:vdm_elbo2_key}
	q_\phi(x_t | x_{t-1}) = q_\phi(x_t | x_{t-1}, x_0) = \frac{q_\phi(x_{t-1} | x_t, x_0)q_\phi(x_t | x_0)}{q_\phi(x_{t-1} | x_0)}
\end{equation}
$$
Plugging this in to the product in the denominator of $$(\ref{eq:vdm_elbo2_deriv1})$$, we get:

$$
\begin{equation}
\label{eq:vdm_elbo2_deriv2}
	\begin{split}
		\log{p(x)} & \geq \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)p_\theta(x_0 | x_1)\prod_{t = 2}^{T}{p_\theta(x_{t-1} | x_t)}}{q_\phi(x_1 | x_0)\prod_{t = 2}^{T}{q_\phi(x_t | x_{t-1})}}] \\
		& = \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)p_\theta(x_0 | x_1)\prod_{t = 2}^{T}{p_\theta(x_{t-1} | x_t)}}{q_\phi(x_1 | x_0)\prod_{t = 2}^{T}{\frac{q_\phi(x_{t-1} | x_t, x_0)q_\phi(x_t | x_0)}{q_\phi(x_{t-1} | x_0)}}}] \\
	\end{split}
\end{equation}
$$
The $$\frac{q_\phi(x_t \mid x_0)}{q_\phi(x_{t-1} \mid x_0)}$$ terms in the denominator product conveniently telescope to $$\frac{q_\phi(x_T \mid x_0)}{q_\phi(x_1 \mid x_0)}$$, which leaves us with the following:

$$
\begin{equation}
\label{eq:vdm_elbo2_deriv3}
	\begin{split}
		\log{p(x)} & \geq \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)p_\theta(x_0 | x_1)\prod_{t = 2}^{T}{p_\theta(x_{t-1} | x_t)}}{q_\phi(x_1 | x_0)\prod_{t = 2}^{T}{\frac{q_\phi(x_{t-1} | x_t, x_0)q_\phi(x_t | x_0)}{q_\phi(x_{t-1} | x_0)}}}] \\
		& = \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)p_\theta(x_0 | x_1)\prod_{t = 2}^{T}{p_\theta(x_{t-1} | x_t)}}{q_\phi(x_T | x_0)\prod_{t = 2}^{T}{q_\phi(x_t | x_{t-1})}}] \\
		& = \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log{p_\theta(x_0 | x_1)}] + \mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p(x_T)}{q_\phi(x_T | x_0)}] \\
		& + \sum_{t=2}^{T}{\mathbb{E}_{q_\phi(x_{1:T} | x_0)}[\log\frac{p_\theta(x_{t-1} | x_t)}{q_\phi(x_{t-1} | x_t, x_0)}]} \\
		& = \mathbb{E}_{q_\phi(x_1 | x_0)}[\log{p_\theta(x_0 | x_1)}] + \mathbb{E}_{q_\phi(x_T | x_0)}[\log\frac{p(x_T)}{q_\phi(x_T | x_0)}] \\
		& + \sum_{t=2}^{T}{\mathbb{E}_{q_\phi(x_{t-1}, x_t | x_0)}[\log\frac{p_\theta(x_{t-1} | x_t)}{q_\phi(x_{t-1} | x_t, x_0)}]} \\
		& = \mathbb{E}_{q_\phi(x_1 | x_0)}[\log{p_\theta(x_0 | x_1)}] - \mathbb{E}_{q_\phi(x_T | x_0)}[\log\frac{q_\phi(x_T | x_0)}{p(x_T)}] \\
		& - \sum_{t=2}^{T}{\mathbb{E}_{q_\phi(x_{t-1}, x_t | x_0)}[\log\frac{q_\phi(x_{t-1} | x_t, x_0)}{p_\theta(x_{t-1} | x_t)}]} \\
	\end{split}
\end{equation}
$$
We can immediately convert the second term $$\mathbb{E}_{q_\phi(x_T \mid x_0)}[\log\frac{q_\phi(x_T \mid x_0)}{p(x_T)}]$$ into a KL divergence $$D_{KL}(q_\phi(x_T \mid x_0) \vert\vert p(x_T))$$. For the terms in the third term sum, we can rewrite $$q_\phi(x_{t - 1}, x_t \mid x_0)$$ as $$q_\phi(x_{t - 1} \mid x_t, x_0)q_\phi(x_t \mid x_0)$$ using the chain rule of probability, so we can also turn these terms into a KL divergence inside an expectation over $$x_t \sim q(x_t \mid x_0)$$: $$\mathbb{E}_{q_\phi(x_t \mid x_0)}[D_{KL}(q_\phi(x_{t-1} \mid x_t, x_0) \vert\vert p_\theta(x_{t-1} \mid x_t))]$$. Thus we get the following ELBO:

$$
\begin{equation}
\label{eq:vdm_elbo2}
	\begin{split}
		\log{p(x)} \geq & \mathbb{E}_{q_\phi(x_1 | x_0)}[\log{p_\theta(x_0 | x_1)}] \\
		& - \sum_{t=2}^{T}{\mathbb{E}_{q_\phi(x_{t}| x_0)}[D_{KL}(q_\phi(x_{t-1} | x_t, x_0) || p_\theta(x_{t-1} | x_t))]} \\
		& - D_{KL}(q_\phi(x_T | x_0) || p(x_T)) \\
	\end{split}
\end{equation}
$$
As with the first ELBO $$(\ref{eq:vdm_elbo1})$$ we derived, we can write the ELBO as a sum of terms $$L_0 + L_1 + \ldots + L_{t - 1} + \ldots + L_T$$:

1. **Reconstruction term** $$L_0 = \mathbb{E}_{q_\phi(x_1 \mid x_0)}[\log{p_\theta(x_0 \mid x_1)}]$$: this term remains unchanged from $$(\ref{eq:vdm_elbo1})$$, and captures how well we can reconstruct the original data $$x_0$$ from the first latent variable $$x_1$$.
2. **Denoising matching terms** $$L_{t - 1} = \mathbb{E}_{q_\phi(x_t \mid x_0)}[D_{KL}(q_\phi(x_{t-1} \mid x_t, x_0) \vert\vert p_\theta(x_{t-1} \mid x_t))]$$: these terms capture how close the denoising transition $$p_\theta(x_{t-1} \mid x_t)$$ matches the "ground truth" denoising transition $$q_\phi(x_{t-1} \mid x_t, x_0)$$. Since we have access to the original data $$x_0$$, we can tractably recover $$x_{t - 1}$$ in the ground truth denoising transition (also called the "forward process posterior").
3. **Prior matching term** $$L_T = D_{KL}(q_\phi(x_T \mid x_0) \vert\vert p(x_T))$$: this expresses how well our forward sampling process $$q(x_T \mid x_0)$$ matches the latent prior $$p(x_T)$$, and is an analogue of the prior matching term in the VAE ELBO and our previous ELBO $$(\ref{eq:vdm_elbo1})$$.

Similarly, we can visualize this ELBO: [TODO: add picture of VDM ELBO 2]

## Denoising Diffusion Probabilistic Models

TODO

### Simplifying the ELBO

We will make a further 2 assumptions to simplify this ELBO:

**Diffusion Model Encoder Assumption.** The latent encoders $$q_\phi(x_t \mid x_{t-1})$$ are fixed and linear Gaussian; that is, each encoding step is a Gaussian centered on the output of the previous encoding step:
$$
q(x_t | x_{t-1}) =  \mathcal{N}(x_t; \sqrt{\alpha_t}x_{t-1}, (1 - \alpha_t)I)
$$
where the noise parameters $\{\alpha_t\}$ are fixed.

Alternatively, we could parameterize the approximate posterior in terms of a variance schedule $$\{\beta_t\}$$ as $$q(x_t \mid x_{t - 1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t}x_0, \beta_tI)$$, with $$\alpha_t = 1 -\beta_t$$. We will stick to using only using $$\alpha_t$$s in our following derivation, but in papers it's common to see a mix of $$\alpha_t$$s and $$\beta_t$$s.

This means we no longer have to learn the parameters $$\phi$$ for the encoders, which makes our learning task (and math) easier, and we will drop the $$\phi$$ subscripts from now on. Furthermore, with respect to our remaining parameters $$\theta$$, the prior matching term $$L_T = D_{KL}(q(x_T \mid x_0) \vert\vert p(x_T))$$ is constant, so we can drop it from our learning objective. (We will revisit the question of how to learn the noise parameters later).

**Diffusion Model Prior Assumption.** The latent encoder variances $$\{\alpha_t\}$$ vary such that the distribution of the final latent variable $$x_T$$ is a standard Gaussian $$\mathcal{N}(0, I)$$.

Now we can interpret what the forward and reverse diffusion processes do. In the forward diffusion process, we start from the original data $$x_0$$ and gradually add noise until it becomes pure Gaussian noise at time $$t = T$$. In the reverse diffusion process, we start from pure Gaussian noise $$x_T \sim \mathcal{N}(0, I)$$ and perform a series of denoising steps $$p_\theta(x_{t - 1} \mid x_t)$$ until we (approximately) recover the original data $$x_0$$.

The dimension, encoder, and prior assumptions are what differentiate a diffusion model from a general MHVAE. Using the prior assumption, we can write our simpler ELBO as

$$
\begin{equation}
\label{eq:vdm_elbo2_simp}
	\begin{split}
		\log{p(x)} \geq & \mathbb{E}_{q(x_1 | x_0)}[\log{p_\theta(x_0 | x_1)}] \\
		& - \sum_{t=2}^{T}{\mathbb{E}_{q(x_{t}| x_0)}[D_{KL}(q(x_{t-1} | x_t, x_0) || p_\theta(x_{t-1} | x_t))]}
	\end{split}
\end{equation}
$$
Recall that by Bayes' rule, we can invert our key insight $$(\ref{eq:vdm_elbo2_key})$$:

$$
\begin{equation}
\label{eq:vdm_elbo2_key_inv}
	q(x_{t-1} | x_t, x_0) = \frac{q(x_t | x_{t-1}, x_0)q(x_{t-1} | x_0)}{q(x_t | x_0)} = \frac{q(x_t | x_{t-1})q(x_{t-1} | x_0)}{q(x_t | x_0)}
\end{equation}
$$
where the second equality holds due to the Markov property. Since we already know $$q(x_t \mid x_{t-1})$$ by the encoder assumption, we only need to figure out $$q(x_t \mid x_0)$$.

Using the reparameterization trick for Gaussian distributions, we can write $$q(x_t \mid x_{t-1})$$ as

$$
\begin{equation}
\label{eq:reparam_trick}
	q(x_{t} | x_{t-1}) = \sqrt{\alpha_t}x_{t-1} + \sqrt{1 - \alpha_t}\epsilon{t-1}
\end{equation}
$$
where $$\epsilon_{t-1}$$ is a standard Gaussian. Rewriting $$x_{t-1}$$ with the reparameterization trick as well results in the following:

$$
\begin{equation}
\label{eq:reparam_ind1}
	\begin{split}
		x_t & = \sqrt{\alpha_t}x_{t-1} + \sqrt{1 - \alpha_t}\epsilon_{t-1} \\
		& = \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}x_{t-2} + \sqrt{1 - \alpha_{t-1}}\epsilon_{t-2}) + \sqrt{1 - \alpha_t}\epsilon_{t-1} \\
		& = \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{\alpha_t(1 - \alpha_{t-1})}\epsilon_{t-2} + \sqrt{1 - \alpha_t}\epsilon_{t-1} \\
	\end{split}
\end{equation}
$$
Since the sum of two independent Gaussian random variables is again a Gaussian random variable whose mean is the sum of the means and whose variance is the sum of the variances, we can simplify this to

$$
\begin{equation}
\label{eq:reparam_ind2}
	\begin{split}
		x_t & = \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{\alpha_t(1 - \alpha_{t-1})}\epsilon_{t-2} + \sqrt{1 - \alpha_t}\epsilon_{t-1} \\
		& = \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{(\sqrt{\alpha_t(1 - \alpha_{t-1})})^2 + (\sqrt{1 - \alpha_t})^2}\epsilon_{t-2} \\
		& = \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{1 - \alpha_t\alpha_{t-1}}\epsilon_{t-2} \\
		& = \ldots \\
		& = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon_0
	\end{split}
\end{equation}
$$
where $$\bar{\alpha}_t = \prod_{i = 1}^{t}{\alpha_i}$$.

Since we have a closed form expression for the forward sampling process $$q(x_t \mid x_0)$$, this means that we don't have to run the diffusion process forward one step at a time; instead, we can directly sample $$x_t$$ from $$\mathcal{N}(\sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)I)$$.

Continuing our derivation for ground truth posterior $$q(x_{t-1} \mid x_t, x_0)$$ we can then plug this into $$(\ref{eq:vdm_elbo2_key_inv})$$ to get

$$
\begin{equation}
	\begin{split}
		q(x_{t-1} | x_t, x_0) & = \frac{q(x_t | x_{t-1})q(x_{t-1} | x_0)}{q(x_t | x_0)} \\
		& = \frac{\mathcal{N}(x_t; \sqrt{\alpha_t}, (1 - \alpha_t)I)\mathcal{N}(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}}, (1 - \bar{\alpha}_{t-1})I)}{\mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}, (1 - \bar{\alpha}_t)I)} \\
		& = \ldots \mbox{(fiddly algebra)} \ldots \\
		& \propto \mathcal{N}(x_{t-1}; \frac{\sqrt{\alpha_{t}}(1 - \bar{\alpha}_{t-1})x_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)x_0}{1 - \bar{\alpha}_t}, \frac{(1 - \alpha_t)(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t})
	\end{split}
\end{equation}
$$
Define

$$
\begin{equation}
\label{eq:denoising_mean}
	\mu_q(x_t, x_0) = \frac{\sqrt{\alpha_{t}}(1 - \bar{\alpha}_{t-1})x_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)x_0}{1 - \bar{\alpha}_t}
\end{equation}
$$

$$
\begin{equation}
\label{eq:denoising_var}
	\sigma_q^2(t) = \frac{(1 - \alpha_t)(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}
\end{equation}
$$
Since we know that our ground truth denoising transition $$q(x_{t-1} \mid x_t, x_0)$$ is a Gaussian with a fixed variance $\sigma_q^2(t)$ (since it only depends on the $$\{\alpha_t\}$$), we can model our denoising step $$p_\theta(x_{t-1} \mid x_t)$$ as the following:

$$
\begin{equation}
\label{eq:denoising_trans_dist}
	p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_q^2(t))
\end{equation}
$$
where $$\mu_\theta(x_t, t)$$ is a function approximator (e.g. a neural network) of the denoising mean, parameterized by $$\theta$$.

Using the formula for the KL divergence of two Gaussians, and using the fact that the variances are equal, we see that the expression collapses into something that looks like a reconstruction loss:

$$
\begin{equation}
\label{eq:kl_div_denoise}
	D_{KL}(q(x_{t-1} | x_t, x_0) || p_\theta(x_{t-1} | x_t)) = \frac{1}{2\sigma_q^2}||\mu_\theta - \mu_q(x_t, x_0)||_2^2
\end{equation}
$$
We can further simplify our function approximator $$\mu_\theta(x_t, t)$$ by parameterizing it to match the form of our ground truth mean as given by $$(\ref{eq:denoising_mean})$$:

$$
\begin{equation} \label{denoising_mean_fn_approx}
	\mu_\theta(x_t, t) = \frac{\sqrt{\alpha_{t}}(1 - \bar{\alpha}_{t-1})x_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)\hat{x}_\theta(x_t, t)}{1 - \bar{\alpha}_t}
\end{equation}
$$
where $$\hat{x}_\theta(x_t, t)$$ tries to predict the original data $$x_0$$ from a noisified version $$x_t$$ and a time index $$t$$.

Putting everything together, we can express the denoising matching loss as

$$
\begin{equation}
\label{eq:loss_data_recon_weighted}
	\begin{split}
		& \argmin_\theta{\sum_{t=2}^{T}{\mathbb{E}_{q(x_t | x_0)}[D_{KL}(q(x_{t-1} | x_t, x_0) || p_\theta(x_{t-1} | x_t))]}} \\
		= & \argmin_\theta{\mathbb{E}_{t \sim U\{2, T\}}[\mathbb{E}_{q(x_t | x_0)}[\frac{1}{2\sigma_q^2}\frac{\bar{\alpha}_{t-1}(1 - \alpha_t)^2}{(1 - \bar{\alpha}_t)^2}||\hat{x}_\theta(x_t, t) - x_0||_2^2]]}
	\end{split}
\end{equation}
$$
We can approximate the expectations with samples: first, we sample a random noise level $$t$$, and then we randomly sample $$x_t \sim q(x_t | x_0)$$, whose distribution is fixed and can be calculated in closed form by $$(\ref{eq:reparam_ind2})$$. Finally, we can use the sampled $$x_t$$ and $$t$$ to calculate a single-sample estimate of the loss via $$\vert\vert\hat{x}_\theta(x_t, t) - x_0\vert\vert_2^2$$.

### Learning Noise Instead of Signal

Recall that by equation $$(\ref{eq:reparam_ind2})$$, we have

$$
x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon_0
$$

We can rearrange this to get

$$
\begin{equation}
\label{eq:reparam_ind_noise}
	x_0 = \frac{x_t - \sqrt{1 - \bar{\alpha}}\epsilon_0}{\sqrt{\bar{\alpha}_t}}
\end{equation}
$$

We can use this expression to reparameterize our ground truth denoising mean $$\mu_q(x_t, x_0)$$ as a function of $$x_t$$ and $$\epsilon_0$$:

$$
\begin{equation}
\label{eq:denoising_mean_noise}
	\begin{split}
		\mu_q(x_t, \epsilon_0) & = \frac{\sqrt{\alpha_{t}}(1 - \bar{\alpha}_{t-1})x_t + \sqrt{\bar{\alpha}_{t-1}(1-\alpha_t)}x_0}{1 - \bar{\alpha}_t} \\
		& = \frac{1}{\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{(1 - \bar{\alpha}_t)(\alpha_t)}}\epsilon_0
	\end{split}
\end{equation}
$$

Analogously, we can set our function approximation $$\mu_\theta(x_t, t)$$ to have the form

$$
\begin{equation}
\label{eq:denoising_mean_noise_fn_approx}
	\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{(1 - \bar{\alpha}_t)(\alpha_t)}}\hat{\epsilon}_\theta(x_t, t)
\end{equation}
$$

So we can rewrite out denoising matching loss as

$$
\begin{equation}
\label{eq:loss_noise_recon}
	\mbox{denoising loss} = \mathbb{E}_{t \sim U\{2, T\}}[\frac{1}{2\sigma_q^2}\frac{(1 - \alpha_t)^2}{(1 - \bar{\alpha}_t)\alpha_t}\mathbb{E}_{q(x_t | x_0)}[||\hat{\epsilon}_\theta(x_t, t) - \epsilon_0||_2^2]]
\end{equation}
$$
where we learn to predict the source noise $$\epsilon_0 \sim \mathcal{N}(\epsilon; 0, 1)$$ using our function approximator $$\hat{\epsilon}_\theta(x_t, t)$$.

Although learning a model $$\hat{x}_\theta(x_t, t)$$ to reconstruct the original data $$x_0$$ and learning a model $$\hat{\epsilon}_\theta(x_t, t)$$ to reconstruct the source noise $$\epsilon_0$$ are mathematically equivalent, the DDPM paper found that modeling the source noise achieves better performance empirically. (More details are in the next section).

### Wait, what about the reconstruction term?

[TODO: add details about VAE reconstruction decoders.]

So far, we've been focused on the denoising matching term and not the reconstruction loss term. We could model the reconstruction loss explicitly, but in the DDPM paper, the authors found that the following simplified objective works better:

$$
\begin{equation} 
\label{eq:vdm_obj_simp}
	L_{\mbox{simple}}(\theta) = \mathbb{E}_{t \sim U[1, t]}[\mathbb{E}_{\epsilon_0 \sim \mathcal{N}(0, I)}[||\hat{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon_0, t) - \epsilon_0||_2^2]]
\end{equation}
$$

In this objective, the $$t = 1$$ case corresponds to the reconstruction loss term in the ELBO. For the $$t > 1$$ terms, this can be viewed as an weighted version of the original variational lower bound $$(\ref{eq:loss_noise_recon})$$. Relative to the orignal loss, we increase the relative weight of the terms with higher noise levels $$t$$, which are more difficult to learn, which the authors say account for its improved performance. As usual, we can optimize this objective using Monte Carlo samples: [TODO: add picture of Algorithm 1 in DDPM paper]

In the DDPM paper, the authors found that modeling the original data and modeling the source noise using the original loss functions performed about equally well, but modeling the source noise with the simplified objective above results in a noticeable improvement in generation quality. Interestingly, dropping the time-dependent coefficients from the $$L_t$$ terms doesn't seem to work when modeling the original data.

## Summary of DDPM Math

A variational diffusion model can be thought of as a Markovian hierarchical variational autoencoder (MHVAE) with the following restrictions:

1. **Diffusion Model Latent Dimension Assumption**: the dimension of the latent variables $$z_{1:T}$$ equals the dimension of the data variable $$x$$. This allows us to consider the latent variables as representing "noisified" versions $$x_{1:T}$$ of the original data $$x_0$$.
2. **Diffusion Model Encoder Assumption**: The encoders $$q_\phi(x_t \mid x_{t-1})$$ are fixed, not learned, and are modeled as a Gaussian centered on the output of the previous encoding step:
$$
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t}x_{t-1}, (1 - \alpha_t)I)
$$
We can think of the noise parameters $$\{\alpha_t\}$$ as being hyperparameters of the model; it is possible to extend the model to learn them.
3. **Diffusion Model Prior Assumption**: The noise parameters $$\{\alpha_t\}$$ vary such that the distribution $$p(x_T)$$ of the final latent variable $$x_T$$ is a standard Gaussian. That is, at the end of the forward diffusion process, the data is "totally random".

With these assumptions, we can derive a variational lower bound of the form $$\log{p(x)} \geq L_0 + L_1 + \ldots L_{t - 1} + \ldots + L_T$$ where

$$
\begin{equation*}
  \begin{split}
    L_0 & = \mathbb{E}_{x_1 \sim q(x_1 | x_0)}[\log{p_\theta(x_0 | x_1)}] \\
    L_{t - 1} & = \mathbb{E}_{x_t \sim q(x_t | x_0)}[D_{KL}(q(x_{t - 1} | x_t, x_0) || p_\theta(x_{t - 1} | x_t))] \\
    L_T & = D_{KL}(q(x_T | x_0) || p(x_T)) \\
  \end{split}
\end{equation*}
$$

Crucially, we can express the data $$x_t$$ at noise level $$t$$ in closed form as a function of the original data $$x_0$$:

$$
x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon_0
$$

where $$\bar{\alpha}_t = \prod_{i=1}^{t}{\alpha_i}$$ and $$\epsilon_0 \sim \mathcal{N}(0, I)$$ is a noise variable. (By the reparameterization trick, this is equivalent to saying that $$q(x_t \mid x_0) \sim \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)I)$$.)

From this and the construction of the encoders in the encoder assumption above, we find that the ground truth denoising transition $$q(x_{t-1} \mid x_t, x_0)$$ is also a Gaussian $$q(x_{t-1} \mid x_t, x_0) \sim \mathcal{N}(x_{t-1}; \mu_q(x_t, x_0), \sigma_q^2(t)I)$$ where

$$
\mu_q(x_t, x_0) = \frac{\sqrt{\alpha_{t}}(1 - \bar{\alpha}_{t-1})x_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)x_0}{1 - \bar{\alpha}_t}
$$

$$
\sigma_q^2(t) = \frac{(1 - \alpha_t)(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}
$$

This suggests that we should model our actual denoising transition $$p_\theta(x_{t-1} \mid x_t)$$ as a Gaussian $$\mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_\theta^2(x_t, t)I)$$ as well. There are several choices of parameterization we can use for our Gaussian distribution. Since our forward posterior variance $$\sigma_q^2(t)$$ depends only on $$t$$, a natural choice is to set the variance $$\sigma_\theta^2(x_t, t)$$ of our denoising transition to be fixed and equal to $$\sigma_q^2(t)$$.

For the denoising mean $$\mu_\theta(x_t, t)$$, one choice of parameterization we can make is to set

$$
\mu_\theta(x_t, t) = \frac{\sqrt{\alpha_{t}}(1 - \bar{\alpha}_{t-1})x_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)\hat{x}_\theta(x_t, t)}{1 - \bar{\alpha}_t}
$$

where $$\hat{x}_\theta(x_t, t)$$ is a model which learns to reconstruct the original data $$x_0$$ from a noisified version $$x_t$$ and the time index $$t$$. Plugging this into our ELBO, we get a denoising objective of the form

$$
\argmin_\theta{\mathbb{E}_{x_t, t}[\frac{1}{2\sigma_q^2(t)}\frac{\bar{\alpha}_{t-1}(1 - \alpha_t)^2}{(1 - \bar{\alpha}_t)^2}||\hat{x}_\theta(x_t, t) - x_0||_2^2}]
$$

where $$x_t \sim q(x_t \mid x_0)$$ and $$t \sim \mbox{Uniform}[2, T]$$.

Alternatively, we can rewrite the ground truth mean in terms of $$x_t$$ and the source noise $$\epsilon_0 \sim \mathcal{N}(0, I)$$:

$$
\mu_q(x_t, \epsilon_0) = \frac{1}{\sqrt{\alpha}_t}(x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha_t}}}\epsilon_0)
$$

This suggests an alternate parameterization

$$
\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha}_t}(x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha_t}}}\hat{\epsilon}_\theta(x_t, t))
$$

where $$\hat{\epsilon}_\theta(x_t, t)$$ is a model that learns to reconstruct the source noise $$\epsilon_0$$ from a noisified version of the data $$x_t$$ and the time index $$t$$. This yields a denoising objective of the form

$$
\argmin_\theta{\mathbb{E}_{x_t, t, \epsilon_0}[\frac{1}{2\sigma_q^2(t)}\frac{(1 - \alpha_t)^2}{\alpha_t(1 - \bar{\alpha}_t)}||\hat{\epsilon}_\theta(x_t, t) - \epsilon_0||_2^2}]
$$

where $$x_t \sim q(x_t \mid x_0)$$, $$t \sim \mbox{Uniform}[2, T]$$, and $$\epsilon_0 \sim \mathcal{N}(0, I)$$.

We could model the log likelihood term $$\log{p_\theta(x_0 \mid x_1)}$$ similarly to how it is modeled for VAEs, but instead it is common to drop the time-dependent coefficients and use the simplified objective

$$
L_{\mbox{simple}}(\theta) = \mathbb{E}_{t, x_0, \epsilon_0}[||\hat{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon_0, t) - \epsilon_0||_2^2]
$$

where $$t \sim \mbox{Uniform}[1, T]$$ and $$\epsilon_0 \sim \mathcal{N}(0, I)$$. The $$t = 1$$ case approximates the log likelihood term $$L_0$$ in the variational lower bound.

For training, we can optimize the above objective by approximating the expectation with samples and using a gradient-based optimizer such as Adam. For sampling, we can run the reverse diffusion process one step at a time, repeatedly applying denoising transitions $$p_\theta(x_{t - 1} \mid x_t)$$ (using the specific parameterization we chose) until we get to the final denoising mean $$\mu_\theta(x_1, 1)$$, which we output as our sample.

## Implementing a DDPM Model

We need the following ingredients to implement a DDPM-style diffusion model:

1. A model $$\hat{x}_\theta(x_t, t)$$ to model the original data or a model $$\hat{\epsilon}_\theta(x_t, t)$$ to model the source noise. Our model needs a way to encode the time index $$t$$ as part of its input.
2. A variance schedule to set the variance parameters $$\{\beta_t\} = \{1 - \alpha_t\}$$. We want the variances to be monotonically increasing throughout the diffusion process, e.g. $$0 < \beta_1 < \beta_2 < \ldots < \beta_T < 1$$.
3. The loss function, which takes samples an arbitrary time index $$t$$ and then samples from $$q(x_t \mid x_0)$$ and calculates the appropriate loss, as described below.

[TODO: expand?]

### Model Architecture

The most popular model architecture for the reverse process is a U-Net architecture, which consists of the following parts:

1. **Downsampling Blocks**: The downsampling portion of the network consists of blocks which downsamples the image width and height and increases the number of channels. Typically, each block contains alternating convolutional layers and nonlinear activations, followed by a downsampling operation.
2. **Bottleneck**: The bottleneck portion of the model performs computation on the (possibly flattened) input at the lowest image resolution (and highest number of feature channels), which occurs in the middle of the model. This computation typically does not change the shape of the incoming tensor..
3. **Upsampling Blocks**: The upsampling portion of the model is essentially a mirror image of the downsampling portion: each upsampling block upsamples the image width and height and decreases the number of channels. Additionally, each upsampling block has a residual connection to the output of the corresponding downsampling block at the same resolution. Typically, each block contains an upsampling operation followed by alternating nonlinear activations and convolutional layers.

The overall U-Net structure then consists of $$N$$ downsample blocks, followed by the bottleneck layer, followed by $$N$$ corresponding upsample blocks. Since the upsampling layers reverse the effect of the downsampling layers, we get an output of the same resolution shape as the input, so we can imagine a U-Net as a kind of autoencoder.

The original U-Net model <d-cite key="https://doi.org/10.48550/arxiv.1505.04597"></d-cite> consisted of the following instantiation of the architecture:

1. **Downsampling block**
	1. **Structure**: (Conv2d 3x3 => ReLU) x 2 => Downsample
	2. **Downsampling operation**: Max pool 2x2
	3. **Resolution downsample**: 2x
	4. **Channel upsample**: 2x
2. **Bottleneck layer**: Identity function (that is, downsampling blocks are followed by upsampling blocks with no intervening computation).
3. **Upsampling block**
	1. **Structure**: Upsample => Concat up/downsample => Conv2d 3x3 => ReLU => Conv2d 3x3
	2. **Upsampling operation**: Conv2d 2x2 up-convolution
	3. **Resolution upsample**: 2x
	4. **Channel downsample**: 2x

[TODO: add picture of original U-Net architecture]

The U-Net in the DDPM paper follows the PixelCNN++ paper <d-cite key="Salimans2017PixeCNN"></d-cite> by using multiple Wide ResNet-style residual blocks <d-cite key="https://doi.org/10.48550/arxiv.1605.07146"></d-cite> in each downsampling and upsampling block. The original Wide ResNet paper used batch normalization and ReLU activations before each convolution, but the DDPM authors instead opt for group normalization <d-cite key="wu2018group"></d-cite> and SiLU activations.

Additionally, the DDPM authors introduce the usage of self-attention <d-cite key="vaswani2017attention"></d-cite> at the 16x16 downsampling/upsampling resolution, as well as a bottleneck layer consisting of two ResNet blocks sandwiching a self-attention block.

[TODO: code block for implementation of attention blocks]

Since our model $$\epsilon_\theta(x_t, t)$$ depends on the time index $$t$$, we need some way to incorporate this information into our model. The DDPM authors accomplish this by using a sinusoidal position embedding very similar to the one used in the Transformer model to encode $$t$$, and then add the embeddings to the each residual block in both the down and upsampling layers.

[TODO: code block for implementation of position embeddings]

Thus, the DDPM U-Net architecture (for the CIFAR-10 dataset, with image resolution (3, 32, 32)) is as follows:

1. **Residual block**: Wide ResNet-like
	1. **Block 1 Structure**: GroupNorm => SiLU => Conv2d 3x3 => Add time embedding
	2. **Block 2 Structure**: GroupNorm => SiLU => Dropout => Conv2d 3x3
	3. Residual connection to block input
2. **Downsampling block**
	1. **Structure**: WideResBlock x 2 => Self-Attention (if using) => Downsample
	2. **Downsampling operation**: Conv2d (3x3) => Max pool 2x2
	3. **Resolution downsample**: 2x
	4. **Channel upsample**: (1, 2, 2, 2)
3. **Bottleneck layer**: WideResBlock => Self-Attention => WideResBlock
4. **Upsampling block**
	1. **Structure**: Concat up/downsample => WideResBlock x 2 => Self-Attention (if using) => Upsample
	2. **Upsampling operation**: Upsample (nearest neighbor) => Conv2d 3x3
	3. **Resolution upsample**: 2x
	4. **Channel downsample**: (2, 2, 2, 1)

In the following implementation of a U-Net, we mostly reproduce the DDPM U-Net architecture, with the following major changes:

1. The residual block is different; we choose to apply post-normalization within the residual blocks and do not use dropout.
2. We apply a pre-GroupNorm attention block in our up/downsample blocks at all resolutions.
3. We use a weight-standardized Conv2d layer, which apparently works well with group normalization.

[TODO: add code block with implementation of U-Net.]

These changes are adopted from subsequent work which seeks to improve the U-Net architecture, such as <d-cite key="https://doi.org/10.48550/arxiv.2105.05233"></d-cite>.

### Variance Schedules

In the DDPM paper, the authors used a fixed (as opposed to learned) variance schedule $$0 < \beta_1 < \ldots < \beta_T < 1$$. The particular hyperparameter settings they used was a linear variance schedule with $$\beta_1 = 10^{-4}$$ and $$\beta_T = 0.02$$, with these specific values chosen such that the prior matching term $$L_T \approx 0$$ and the variances are small relative to the data scaled to $$[-1, 1]$$.

[TODO: insert code block for variance schedule implemented.]

In subsequent work, it was found that the choice of a linear variance schedule could be improved on by using a cosine variance schedule. <d-cite key="https://doi.org/10.48550/arxiv.2102.09672"></d-cite> There are also extensions of the model which allow the variances to be learned jointly with the mean. <d-cite key="https://doi.org/10.48550/arxiv.2107.00630"></d-cite>

### Training

We can give the following batch training algorithm for a DDPM model modeling the source noise $\epsilon_0$ using the simplified training objective $$(\ref{eq:vdm_obj_simp})$$:

[TODO: insert picutre/graphic of the training algorithm (Alg 1 in paper).]
<!-- $$
\begin{algorithm}
	\caption{DDPM Training}
  \label{alg:ddpm_train}
	\begin{algorithmic}[1]
		\Repeat
		\State Sample a mini-batch of data $\{x_{0, i}\}_{i = 1}^{N}$; $x_{0, i} \sim q(x_0)$.
		\State Sample time steps $\{t_i\}_{i = 1}^{N}$; $t_i \sim \mbox{Uniform}(\{1, \ldots, T\})$.
		\State Sample source noise $\{\epsilon_i\}_{i = 1}^{N}$; $\epsilon_i \sim \mathcal{N}(0, I)$.
		\State Take a gradient descent step on
		\nabla_\theta\frac{1}{N}\sum_{i = 1}^{N}{||\epsilon_i - \hat{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}x_{0, i} + \sqrt{1 - \bar{\alpha}_t}\epsilon_i, t_i)||^2}
		\Until{convergence}
	\end{algorithmic}
\end{algorithm}
$$ -->

Essentially, for each data point $$x_0$$ we train on, we sample a time step $$t \sim \mbox{Uniform}(\{1, \ldots, T\})$$ and some source noise $$\epsilon \sim \mathcal{N}(0, I)$$. We then sample from the forward process posterior $$q(x_t \mid x_0) \sim \mathcal{N}(\sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)I)$$ using the reparameterization trick and take a gradient step on the loss term associated with $$t$$, which is the reconstruction loss $$\vert\vert\epsilon - \hat{\epsilon}_\theta(x_t, t)\vert\vert^2$$.

[TODO: insert code block for the training code.]

If we are using a different loss (such as the original variational lower bound loss), we would keep the same structure as Algorithm $$\ref{alg:ddpm_train}$$ with appropriate modifications, mainly to the gradient descent step. 

[TODO: insert code block for the loss function implementation.]

### Sampling

We now know how to train a diffusion model. After training a model, how can we sample from it?

As with other generative models, we start with a noise sample $$x_T \sim \mathcal{N}(x_T; 0, I)$$. We then run the reverse diffusion process one step at a time. By construction, we know that $$p_\theta(x_{t - 1} \mid x_t)$$ has distribution $$\mathcal{N}(\frac{1}{\sqrt{\alpha_t}}(x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}}\hat{\epsilon}_\theta(x_t, t)), \sigma_q^2(t)I)$$. Using the reparameterization trick, we can first sample some noise $$z \sim \mathcal{N}(0, I)$$, and then sample $$x_{t - 1}$$ according to

$$
x_{t - 1} = \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}}\hat{\epsilon}_\theta(x_t, t)) + \sigma_q(t)z
$$

At the last denoising step, we output the denoising mean $$x_0 = \mu_\theta(x_1, 1)$$ without any noise. This process is summarized in Algorithm $$\ref{alg:ddpm_sample}$$.

[TODO: insert picture/graphic of sampling algorithm (Algorithm 2 in paper).]
<!-- $$
\begin{algorithm}
	\caption{DDPM Sampling} \label{alg:ddpm_sample}
	\begin{algorithmic}[1]
		\State Sample noise from prior $x_T \sim p(x_T) = \mathcal{N}(0, I)$.
		\For{t = T, $\ldots$, 1}
		\State Sample noise $z \sim \mathcal{N}(0, I)$ if $t > 1$, else $z = 0$.
		\State Sample from decoding transition $p_\theta(x_{t - 1} | x_t)$:
		x_{t - 1} = \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}}\hat{\epsilon}_\theta(x_t, t)) + \sigma_t z
		\EndFor
		\State \Return $x_0$
	\end{algorithmic}
\end{algorithm}
$$ -->

We can implement the sampling algorithm as follows:

[TODO: insert code block of sampling implementation.]

### Evaluation and Metrics

TODO

## Practical Notes on Training a Diffusion Model

TODO